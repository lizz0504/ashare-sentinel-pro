"""
Guru Watcher - Complete Supabase-Integrated Service

å®Œæ•´çš„å¤§Vä¿¡å·ç›‘æ§æœåŠ¡ï¼šRSSæŠ“å– â†’ AIæå– â†’ Supabaseä¿å­˜
"""

import asyncio
import logging
import feedparser
import httpx
import json
from typing import List, Dict, Any, Optional
from datetime import datetime, timezone, timedelta

from supabase import create_client

from app.core.config import settings
from app.core.llm_factory import LLMFactory
from app.config.guru_sources import GURU_RSS_SOURCES, get_active_gurus

logger = logging.getLogger(__name__)


# ============================================
# LLM æå–æç¤ºè¯
# ============================================

EXTRACTION_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èèˆ†æƒ…åˆ†æå¸ˆï¼Œæ“…é•¿ä»æŠ•èµ„å¤§Vçš„å¸–å­ä¸­æå–ç»“æ„åŒ–äº¤æ˜“ä¿¡å·ã€‚

è¯·åˆ†æä»¥ä¸‹å¸–å­å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯å¹¶ä»¥JSONæ ¼å¼è¿”å›ï¼š

**å¸–å­å†…å®¹**ï¼š
ä½œè€…ï¼š{guru_name}
å¹³å°ï¼š{platform}
å†…å®¹ï¼š{content}

**æå–è¦æ±‚**ï¼š

1. **mentioned_symbols**: æå–æ‰€æœ‰æåˆ°çš„è‚¡ç¥¨ä»£ç 
   - Aè‚¡ä»£ç æ ¼å¼ï¼š6ä½æ•°å­—ï¼ˆå¦‚ 600519ï¼‰
   - ç¾è‚¡ä»£ç ï¼šå¤§å†™å­—æ¯ï¼ˆå¦‚ NVDAï¼‰
   - æ¸¯è‚¡ä»£ç ï¼š5ä½æ•°å­—ï¼ˆå¦‚ 00700ï¼‰
   - è‚¡ç¥¨åˆ«åæ˜ å°„ï¼šèŒ…å°â†’600519, äº”ç²®æ¶²â†’000858, å®å¾·æ—¶ä»£â†’300750
   - è¿”å›æ ¼å¼ï¼š["600519", "NVDA"]

2. **sentiment**: åˆ¤æ–­æ•´ä½“æƒ…ç»ªå€¾å‘
   - "Bullish" - æ˜æ˜¾çœ‹å¤š
   - "Bearish" - æ˜æ˜¾çœ‹ç©º
   - "Neutral" - ä¸­æ€§

3. **action**: æ“ä½œå»ºè®®
   - "BUY" - ä¹°å…¥
   - "SELL" - å–å‡º
   - "HOLD" - æŒæœ‰
   - "COMMENT" - è¯„è®º

4. **summary**: ä¸€å¥è¯æ€»ç»“ï¼ˆä¸è¶…è¿‡50å­—ï¼‰

**è¾“å‡ºæ ¼å¼ï¼ˆçº¯JSONï¼‰**ï¼š
```json
{{
  "mentioned_symbols": ["è‚¡ç¥¨ä»£ç "],
  "sentiment": "Bullish/Bearish/Neutral",
  "action": "BUY/SELL/HOLD/COMMENT",
  "summary": "ä¸€å¥è¯æ€»ç»“"
}}
```

è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—ã€‚
"""


# ============================================
# ä¸»æœåŠ¡ç±»
# ============================================

class GuruWatcherService:
    """å¤§Vä¿¡å·ç›‘æ§æœåŠ¡ï¼ˆSupabaseé›†æˆç‰ˆï¼‰"""

    def __init__(self):
        self.client: Client = None
        self.rss_client = httpx.AsyncClient(timeout=30)
        self._init_client()

    def _init_client(self):
        """åˆå§‹åŒ– Supabase å®¢æˆ·ç«¯"""
        try:
            self.client = create_client(
                settings.SUPABASE_URL,
                settings.SUPABASE_SERVICE_KEY
            )
            logger.info("[GURU] Supabase client initialized")
        except Exception as e:
            logger.error(f"[GURU] Failed to init Supabase client: {e}")

    async def _ai_extract(
        self,
        guru_name: str,
        platform: str,
        content: str
    ) -> Optional[Dict[str, Any]]:
        """
        ä½¿ç”¨ LLM ä»æ–‡æœ¬ä¸­æå–äº¤æ˜“ä¿¡å·
        """
        try:
            prompt = EXTRACTION_PROMPT.format(
                guru_name=guru_name,
                platform=platform,
                content=content
            )

            logger.debug(f"[GURU] Calling LLM for {guru_name}...")
            result = await LLMFactory.fast_reply(
                model="deepseek",
                system="ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èæ–‡æœ¬åˆ†æä¸“å®¶ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ç»“æœã€‚",
                user=prompt,
                timeout=15
            )

            logger.debug(f"[GURU] LLM response for {guru_name}: {result[:200]}...")

            if not result or result.startswith("[é”™è¯¯]"):
                logger.warning(f"[GURU] LLM returned error or empty for {guru_name}")
                return None

            # è§£æ JSON
            parsed = self._parse_json(result)
            if parsed:
                logger.info(f"[GURU] Successfully extracted signal from {guru_name}")
            else:
                logger.warning(f"[GURU] Failed to parse JSON from LLM response for {guru_name}")
            return parsed

        except Exception as e:
            logger.error(f"[GURU] AI extraction error for {guru_name}: {e}")
            import traceback
            logger.error(f"[GURU] Traceback: {traceback.format_exc()}")
            return None

    def _parse_json(self, response: str) -> Optional[Dict]:
        """è§£æ JSON å“åº”"""
        import re

        logger.debug(f"[GURU] LLM Response: {response[:500]}")

        try:
            return json.loads(response)
        except json.JSONDecodeError:
            pass

        # æå– JSON ä»£ç å—
        json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL | re.IGNORECASE)
        if json_match:
            try:
                json_str = json_match.group(1)
                logger.debug(f"[GURU] Extracted JSON from code block: {json_str[:200]}")
                return json.loads(json_str)
            except json.JSONDecodeError as e:
                logger.debug(f"[GURU] Failed to parse JSON from code block: {e}")
                pass

        # æå–èŠ±æ‹¬å·å†…å®¹
        first_brace = response.find('{')
        last_brace = response.rfind('}')
        if first_brace != -1 and last_brace != -1:
            try:
                json_text = response[first_brace:last_brace + 1]
                json_text = re.sub(r',(\s*[}\]])', r'\1', json_text)
                logger.debug(f"[GURU] Extracted JSON from braces: {json_text[:200]}")
                return json.loads(json_text)
            except json.JSONDecodeError as e:
                logger.debug(f"[GURU] Failed to parse JSON from braces: {e}")
                pass

        logger.error(f"[GURU] Failed to parse JSON from response: {response[:300]}")
        return None

    async def fetch_and_save_all(
        self,
        guru_names: Optional[List[str]] = None,
        limit_per_guru: int = 3
    ) -> Dict[str, Any]:
        """
        éå†æ‰€æœ‰å¤§Vï¼ŒæŠ“å–RSSã€æå–ä¿¡å·ã€ä¿å­˜åˆ°æ•°æ®åº“

        Args:
            guru_names: å¤§Våå­—åˆ—è¡¨
            limit_per_guru: æ¯ä¸ªå¤§Vè·å–çš„æ•°é‡é™åˆ¶

        Returns:
            é‡‡é›†ç»Ÿè®¡ä¿¡æ¯
        """
        if guru_names is None:
            active_gurus = get_active_gurus()
            guru_names = [g.name for g in active_gurus]

        logger.info(f"[GURU] ğŸ¦… Scanning {len(guru_names)} gurus")

        stats = {
            "total_gurus": len(guru_names),
            "successful_gurus": 0,
            "feeds_fetched": 0,
            "signals_extracted": 0,
            "signals_saved": 0,
            "errors": [],
            "saved_summaries": []
        }

        for guru_name in guru_names:
            try:
                # è·å–å¤§Vé…ç½®
                guru_config = None
                for g in GURU_RSS_SOURCES:
                    if g.name == guru_name:
                        guru_config = g
                        break

                if not guru_config:
                    logger.warning(f"[GURU] Guru not found: {guru_name}")
                    continue

                # 1. æŠ“å– RSS
                feed = await self._fetch_rss(guru_config.rss_url)
                if not feed or not feed.entries:
                    logger.warning(f"[GURU] No entries for {guru_name}")
                    continue

                stats["feeds_fetched"] += len(feed.entries[:limit_per_guru])

                # 2. å¤„ç†æ¡ç›®
                for entry in feed.entries[:limit_per_guru]:
                    try:
                        link = entry.get('link', '')
                        if not link:
                            continue

                        # 3. å»é‡
                        existing = self.client.table("guru_signals").select("id").eq("source_link", link).execute()
                        if existing.data:
                            continue

                        # 4. æå–å†…å®¹
                        title = entry.get('title', '')
                        description = entry.get('description', '')
                        content = f"{title}\n{description}"

                        # 5. AI æå–
                        ai_result = await self._ai_extract(
                            guru_name=guru_name,
                            platform=guru_config.platform,
                            content=content
                        )

                        if not ai_result:
                            continue

                        stats["signals_extracted"] += 1

                        # 6. ä¿å­˜åˆ°æ•°æ®åº“
                        signal_data = {
                            "guru_name": guru_name,
                            "platform": guru_config.platform,
                            "source_link": link,
                            "source_id": f"{guru_config.uid}_{hash(link)}",
                            "raw_text": content,
                            "publish_time": self._parse_date(entry.get('published')),
                            "mentioned_symbols": ai_result.get("mentioned_symbols", []),
                            "sentiment": ai_result.get("sentiment", "Neutral"),
                            "action": ai_result.get("action", "COMMENT"),
                            "summary": ai_result.get("summary", ""),
                            "related_themes": ai_result.get("related_themes", []),
                            "key_factors": ai_result.get("key_factors", []),
                            "confidence_score": 0.8,
                        }

                        result = self.client.table("guru_signals").upsert(
                            signal_data,
                            on_conflict="source_link"
                        ).execute()

                        if result.data:
                            stats["signals_saved"] += 1
                            summary = ai_result.get("summary", "")
                            stats["saved_summaries"].append(f"{guru_name}: {summary}")
                            logger.info(f"[GURU] âœ… {guru_name} -> {summary}")

                    except Exception as e:
                        logger.error(f"[GURU] Error processing entry: {e}")

                stats["successful_gurus"] += 1

            except Exception as e:
                logger.error(f"[GURU] Error processing {guru_name}: {e}")
                stats["errors"].append(f"{guru_name}: {str(e)}")

        logger.info(f"[GURU] Collection complete: {stats}")
        return stats

    async def _fetch_rss(self, rss_url: str) -> Optional[feedparser.FeedParserDict]:
        """æŠ“å– RSS feed"""
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                "Accept": "application/rss+xml, application/xml, text/xml, */*",
            }

            response = await self.rss_client.get(rss_url, headers=headers, follow_redirects=True)
            response.raise_for_status()

            return feedparser.parse(response.content)

        except Exception as e:
            logger.error(f"[GURU] RSS fetch error: {e}")
            return None

    def _parse_date(self, date_obj) -> Optional[str]:
        """è§£ææ—¥æœŸ"""
        if not date_obj:
            return None

        try:
            if hasattr(date_obj, 'isoformat'):
                return date_obj.isoformat()
            return str(date_obj)
        except Exception:
            return None

    async def get_recent_signals(self, limit: int = 20) -> List[Dict]:
        """è·å–æœ€è¿‘çš„ä¿¡å·"""
        try:
            result = self.client.table("guru_signals").select("*").order("created_at", desc=True).limit(limit).execute()
            return result.data if result.data else []
        except Exception as e:
            logger.error(f"[GURU] Error getting signals: {e}")
            return []

    async def get_aggregated_sentiment(self, symbol: str) -> Dict[str, Any]:
        """è·å–è‚¡ç¥¨èšåˆæƒ…ç»ª"""
        try:
            result = self.client.table("guru_signals").select("*").contains("mentioned_symbols", [symbol]).execute()

            if not result.data:
                return {"symbol": symbol, "total_signals": 0, "avg_sentiment": "Neutral"}

            signals = result.data
            bullish = sum(1 for s in signals if s.get("sentiment") == "Bullish")
            bearish = sum(1 for s in signals if s.get("sentiment") == "Bearish")
            total = len(signals)

            if total == 0:
                avg_sentiment = "Neutral"
            elif bullish / total > 0.6:
                avg_sentiment = "Strongly Bullish"
            elif bullish / total > 0.4:
                avg_sentiment = "Bullish"
            elif bearish / total > 0.4:
                avg_sentiment = "Bearish"
            else:
                avg_sentiment = "Neutral"

            return {
                "symbol": symbol,
                "total_signals": total,
                "bullish_count": bullish,
                "bearish_count": bearish,
                "neutral_count": total - bullish - bearish,
                "avg_sentiment": avg_sentiment
            }
        except Exception as e:
            logger.error(f"[GURU] Error getting sentiment: {e}")
            return {"symbol": symbol, "error": str(e)}

    async def close(self):
        """å…³é—­èµ„æº"""
        await self.rss_client.aclose()


# ============================================
# å•ä¾‹å®ä¾‹
# ============================================

_guru_watcher_instance: Optional[GuruWatcherService] = None


def get_guru_watcher_service() -> GuruWatcherService:
    """è·å– GuruWatcher å•ä¾‹"""
    global _guru_watcher_instance
    if _guru_watcher_instance is None:
        _guru_watcher_instance = GuruWatcherService()
    return _guru_watcher_instance


# ============================================
# ä¸»å…¥å£å’Œæµ‹è¯•
# ============================================

async def main():
    """æµ‹è¯•å…¥å£"""
    print("=" * 60)
    print("Guru Watcher - Complete Service Test")
    print("=" * 60)

    service = get_guru_watcher_service()

    # æµ‹è¯•å®Œæ•´é‡‡é›†å‘¨æœŸ
    print("\n[TEST] Running collection cycle...")
    stats = await service.fetch_and_save_all(
        guru_names=["ä½†æ–Œ", "é€¸ä¿®", "å¢æ¡‚å‡¤"],
        limit_per_guru=2
    )

    print(f"\n[TEST] Results:")
    print(f"  Total gurus: {stats['total_gurus']}")
    print(f"  Successful: {stats['successful_gurus']}")
    print(f"  Feeds fetched: {stats['feeds_fetched']}")
    print(f"  Signals extracted: {stats['signals_extracted']}")
    print(f"  Signals saved: {stats['signals_saved']}")

    if stats['saved_summaries']:
        print(f"\n[TEST] Saved signals:")
        for summary in stats['saved_summaries']:
            print(f"  - {summary}")

    # æµ‹è¯•è·å–ä¿¡å·
    print(f"\n[TEST] Fetching recent signals...")
    signals = await service.get_recent_signals(limit=5)
    print(f"  Retrieved {len(signals)} signals")

    # æµ‹è¯•èšåˆæƒ…ç»ª
    print(f"\n[TEST] Aggregated sentiment for 600519...")
    sentiment = await service.get_aggregated_sentiment("600519")
    print(f"  Total: {sentiment['total_signals']}")
    print(f"  Avg sentiment: {sentiment['avg_sentiment']}")

    await service.close()

    print("\n" + "=" * 60)
    print("Test Complete!")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
