# -*- coding: utf-8 -*-
"""
Search Service - å¢å¼ºç‰ˆç½‘ç»œæ£€ç´¢æœåŠ¡
ä½¿ç”¨ Tavily Search API è·å–æœ€æ–°æ–°é—»ã€ç ”æŠ¥å’Œå¸‚åœºæƒ…æŠ¥ï¼Œè§£å†³ AI å¹»è§‰é—®é¢˜

ä¼˜åŒ–éœ€æ±‚ï¼š
1. Query Pre-processingï¼šå°†åŸå§‹æŸ¥è¯¢ç”¨ç«™ç‚¹å‚æ•°åŒ…è£…
2. å¼ºåˆ¶é«˜çº§æœç´¢ï¼šsearch_depth="advanced"
3. ä¸¥æ ¼æ—¶é—´çª—è¿‡æ»¤ï¼šä¿ç•™ days=7 ä½†æ·»åŠ  include_raw_content=True
4. å†…å®¹å»å™ªï¼šå‰”é™¤åŒ…å«"è¡Œæƒ…"ã€"å®æ—¶è¡Œæƒ…"ã€"ä¸ªè‚¡æ¦‚å†µ"ã€"èµ„é‡‘æµå‘"çš„ç»“æœ
5. ç»“æœåˆ†å±‚ï¼šå¦‚æœå°‘äº3æ¡ç»“æœï¼Œæ˜ç¡®æ ‡æ³¨"éå®æ—¶ä¿¡æ¯"
6. æ—¥æœŸæ ¼å¼è§„èŒƒåŒ–ï¼šåŒ¹é…æ­£æ–‡ä¸­æ—¥æœŸæ ¼å¼ï¼ˆå¦‚"2026å¹´2æœˆ"ã€"2026-02"ï¼‰
"""

import os
import asyncio
import re
from typing import Dict, Optional
from datetime import datetime, timedelta
from tavily import TavilyClient

_tavily_client = None

# ============================================
# å…¨å±€é…ç½®
# ============================================

# æ”¯æŒçš„æœç´¢ç«™ç‚¹é…ç½®
SITE_CONFIG = {
    "eastmoney.com": {"name": "ä¸œæ–¹è´¢å¯Œ", "query_prefix": "site:eastmoney.com "},
    "xueqiu.com": {"name": "é›ªçƒ", "query_prefix": "site:xueqiu.com "},
    "sina.com.cn": {"name": "æ–°æµªè´¢ç»", "query_prefix": "site:sina.com.cn "},
    "10jqka.com.cn": {"name": "åŒèŠ±é¡º", "query_prefix": "site:10jqka.com.cn "},
    "cs.com.cn": {"name": "ä¸­è¯ç½‘", "query_prefix": "site:cs.com.cn "},
    "stock.stcn.com": {"name": "å·¨æ½®èµ„è®¯", "query_prefix": "site:stock.stcn.com "},
    "sse.com.cn": {"name": "ä¸Šäº¤æ‰€", "query_prefix": "site:sse.com.cn "},
    "szse.cn": {"name": "æ·±äº¤æ‰€", "query_prefix": "site:szse.cn "},
    "cninfo.com.cn": {"name": "ä¸­è¯ç½‘", "query_prefix": "site:cninfo.com.cn "},
}

# æ¥æºä¼˜å…ˆçº§é…ç½® (0-1, è¶Šé«˜è¶Šä¼˜å…ˆ)
SOURCE_PRIORITIES = {
    "cninfo.com.cn": 1.0,     # å·¨æ½®èµ„è®¯ (å®˜æ–¹å…¬å‘Š)
    "sse.com.cn": 1.0,          # ä¸Šäº¤æ‰€ (å®˜æ–¹)
    "szse.cn": 1.0,           # æ·±äº¤æ‰€ (å®˜æ–¹)
    "stock.stcn.com": 0.95,     # å·¨æ½®èµ„è®¯ (å…¬å‘Š)
    "cs.com.cn": 0.8,           # ä¸­è¯ç½‘ (æƒå¨)
    "eastmoney.com": 0.7,       # ä¸œæ–¹è´¢å¯Œ
    "10jqka.com.cn": 0.6,      # åŒèŠ±é¡º
    "sina.com.cn": 0.7,         # æ–°æµªè´¢ç»
    "xueqiu.com": 0.4,          # é›ªçƒ (ç”¨æˆ·ç”Ÿæˆå†…å®¹)
}

# å™ªå£°å…³é”®è¯ï¼ˆç”¨äºå†…å®¹å»å™ªï¼‰
NOISE_KEYWORDS = [
    "è¡Œæƒ…", "å®æ—¶è¡Œæƒ…", "ä¸ªè‚¡æ¦‚å†µ", "èµ„é‡‘æµå‘",
    "æœºæ„è¯„çº§", "æ™¨ä¼šæ—©æŠ¥", "åˆé—´å…¬å‘Š",
    "é¾™è™æ¦œ", "æ¦‚å¿µæ¿å—", "çƒ­ç‚¹è¿½è¸ª",
    "å¤§å®—äº¤æ˜“", "èèµ„èåˆ¸", "è‚¡æŒ‡æœŸè´§",
    "æ¸¯è‚¡é€š", "åŒ—å‘èµ„é‡‘", "å€ºå¸‚", "åŸº"
]

# ============================================
# Tavily å®¢æˆ·ç«¯åˆå§‹åŒ–
# ============================================


def _get_tavily_client():
    """è·å–æˆ–åˆ›å»º Tavily å®¢æˆ·ç«¯"""
    global _tavily_client
    if _tavily_client is None:
        try:
            # ç¡®ä¿åŠ è½½.envæ–‡ä»¶
            from dotenv import load_dotenv
            load_dotenv()
            api_key = os.getenv("TAVILY_API_KEY")

            if not api_key:
                print(
                    "[SEARCH] ERROR: TAVILY_API_KEY not found "
                    "in environment"
                )
                return None

            # éªŒè¯API keyæ ¼å¼
            if not api_key.startswith('tvly-'):
                print(
                    "[SEARCH] WARNING: API key format may be invalid "
                    "(should start with 'tvly-')"
                )
                return None

            # åˆå§‹åŒ–å®¢æˆ·ç«¯
            print(
                f"[SEARCH] Tavily API Key found (length: {len(api_key)}, "
                f"prefix: {api_key[:10]}...)"
            )
            _tavily_client = TavilyClient(api_key=api_key)
            print("[SEARCH] Tavily Search client initialized successfully")

        except ImportError:
            print(
                "[SEARCH] ERROR: tavily-python not installed. "
                "Run: pip install tavily-python"
            )
            return None
        except Exception as e:
            print(f"[SEARCH] ERROR: Failed to init Tavily client: {e}")
            _tavily_client = None

    return _tavily_client


# ============================================
# æŸ¥è¯¢é¢„å¤„ç†
# ============================================


def _preprocess_query(
    symbol: str,
    stock_name: str,
    query_type: str = "news"
) -> str:
    """
    é¢„å¤„ç†æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œæ·»åŠ ç«™ç‚¹å‚æ•°å’Œé«˜çº§æœç´¢é€‰é¡¹

    Args:
        symbol: è‚¡ç¥¨ä»£ç 
        stock_name: è‚¡ç¥¨åç§°
        query_type: æŸ¥è¯¢ç±»å‹ (news/company)

    Returns:
        å¢å¼ºåçš„æŸ¥è¯¢å­—ç¬¦ä¸²
    """
    # ç¡®å®šæŸ¥è¯¢ç±»å‹
    if query_type not in ["news", "company"]:
        query_type = "news"

    # è·å–æ—¶é—´çª—ï¼ˆæœ€è¿‘7å¤©ï¼‰
    end_date = datetime.now()
    start_date = end_date - timedelta(days=7)
    date_range = (
        f"{start_date.strftime('%Y-%m-%d')} TO "
        f"{end_date.strftime('%Y-%m-%d')}"
    )

    # æ„å»ºåŸºç¡€æŸ¥è¯¢
    base_query = (
        f"{stock_name} {symbol} {stock_name} "
        "æœ€æ–° ç ”æŠ¥ ä¸šç»© åˆ©å¥½ åˆ©ç©º"
    )

    # æ ¹æ®æŸ¥è¯¢ç±»å‹ä½¿ç”¨ä¸åŒç­–ç•¥
    if query_type == "news":
        # æ–°é—»æœç´¢ï¼šè¦æ±‚æ·±åº¦æœç´¢ï¼ŒåŒ…å«åŸå§‹å†…å®¹
        sites = "eastmoney.com OR xueqiu.com OR sina.com.cn "
        sites += "OR 10jqka.com.cn OR cs.com.cn"
        return f'({base_query} ({date_range}) {{site: {sites}}})'
    elif query_type == "company":
        # å…¬å¸ä¿¡æ¯ï¼šåŸºç¡€æœç´¢å³å¯
        return f'"{stock_name} {symbol} ä¸»è¥ä¸šåŠ¡ è¡Œä¸š ç®€ä»‹ å…¬å¸èµ„æ–™"'


# ============================================
# å†…å®¹è´¨é‡è¯„åˆ†ï¼ˆç”¨äºå»å™ªï¼‰
# ============================================

def _calculate_content_quality_score(title: str, content: str) -> float:
    """
    è®¡ç®—å†…å®¹è´¨é‡åˆ†æ•° (0-1)ï¼Œåˆ†æ•°è¶Šé«˜è¶Šå¯èƒ½æ˜¯æœ‰ä»·å€¼çš„æ–°é—»

    è¯„åˆ†è§„åˆ™ï¼š
    - æ ‡é¢˜åŒ…å«æ ¸å¿ƒå…³é”®è¯ (+0.3)
    - å†…å®¹é•¿åº¦é€‚ä¸­ (+0.2)
    - æ ‡é¢˜æ ¼å¼è§„èŒƒ (+0.1)
    - æ¥æºå¯ä¿¡åº¦ (+0.2)
    - æ—¶æ•ˆæ€§ (+0.2ï¼Œ7å¤©å†…å†…å®¹ +0.3
    """
    score = 0.0

    # æ ‡é¢˜è´¨é‡ (0.3)
    if title and len(title) >= 5 and len(title) <= 30:
        score += 0.3

    # å†…å®¹é•¿åº¦ (0.2) - é€‚ä¸­é•¿åº¦æ›´æœ‰ä»·å€¼
    if content and 200 <= len(content) <= 1000:
        score += 0.2
    elif len(content) > 1000:
        score += 0.1

    # æ ‡é¢˜æ ¼å¼ (0.1) - åŒ…å«è‚¡ç¥¨ä»£ç æˆ–æ•°å­—
    if any(char.isdigit() for char in title):
        score += 0.1

    # æ¥æºå¯ä¿¡åº¦ (0.2) - æ¥è‡ªä¸»æµè´¢ç»ç½‘ç«™
    credible_sources = [
        "eastmoney.com",
        "xueqiu.com",
        "sina.com.cn",
        "10jqka.com.cn"
    ]
    if any(source in content for source in credible_sources):
        score += 0.2

    # æ—¶æ•ˆæ€§ (0.3) - 7å¤©å†…å†…å®¹
    # æå–å‘å¸ƒæ—¥æœŸåˆ¤æ–­
    try:
        pub_date_match = re.search(r'(\d{4})-(\d{1,2})-(\d{1,2})', content)
        if pub_date_match:
            pub_date = datetime.strptime(pub_date_match.group(0), '%Y-%m-%d')
            days_diff = (datetime.now() - pub_date).days
            if days_diff <= 7:
                score += 0.3
    except Exception:
        pass

    return min(score, 1.0)


# ============================================
# å™ªå£°æ£€æµ‹
# ============================================

def _is_noise_content(title: str, content: str) -> bool:
    """
    æ£€æµ‹å†…å®¹æ˜¯å¦ä¸ºå™ªéŸ³ï¼ˆåŒ…å«è¡Œæƒ…å…³é”®è¯ï¼‰

    Returns:
        True if æ˜¯å™ªéŸ³ï¼ŒFalse if ä¸æ˜¯å™ªéŸ³
    """
    # æ£€æŸ¥æ˜¯å¦åŒ…å«å™ªéŸ³å…³é”®è¯
    noise_keywords = NOISE_KEYWORDS

    # æ ‡é¢˜æ£€æµ‹
    title_lower = title.lower()
    for keyword in noise_keywords:
        if keyword in title_lower:
            return True

    # å†…å®¹æ£€æµ‹ï¼ˆæ›´ä¸¥æ ¼ï¼‰
    content_lower = content.lower()
    for keyword in noise_keywords:
        if keyword in content_lower:
            return True

    # ç‰¹æ®Šæƒ…å†µï¼šå¦‚æœæ˜¯"ä¸ªè‚¡è¡Œæƒ…"è¿™ç±»æ˜ç¡®å™ªéŸ³ï¼Œå³ä½¿æ ‡é¢˜ä¸å«å…³é”®è¯ä¹Ÿè¦è¿‡æ»¤
    if any(kw in title_lower for kw in ["ä¸ªè‚¡", "è¡Œæƒ…", "èµ„é‡‘æµ"]):
        return True

    return False


# ============================================
# æ—¥æœŸæ ¼å¼è§„èŒƒåŒ–
# ============================================

def _extract_and_normalize_date(content: str) -> Optional[str]:
    """
    ä»æ–‡ç« å†…å®¹ä¸­æå–å¹¶è§„èŒƒåŒ–æ—¥æœŸæ ¼å¼

    æ”¯æŒçš„æ ¼å¼ï¼š
    - "2026å¹´2æœˆ"  ->  2026-02
    - "2026-02"     -> 2026-02

    Returns:
        è§„èŒƒåŒ–çš„æ—¥æœŸå­—ç¬¦ä¸²ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å› None
    """
    # ä¼˜å…ˆå°è¯•åŒ¹é…å¸¸è§ä¸­æ–‡æ—¥æœŸæ ¼å¼
    date_patterns = [
        (r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', 3),  # 2024å¹´02æœˆ15æ—¥
        (r'(\d{4})-(\d{1,2})-(\d{1,2})', 3),      # 2024-02-15
        (r'(\d{4})/(\d{1,2})/(\d{1,2})', 3),      # 2024/02/15
        (r'(\d{4})å¹´(\d{1,2})æœˆ', 2),              # 2024å¹´02æœˆ
    ]

    for pattern, group_count in date_patterns:
        match = re.search(pattern, content)
        if match:
            try:
                year = match.group(1)
                month = match.group(2).lstrip('0') or '1'
                if group_count >= 3:
                    day = match.group(3).lstrip('0') or '1'
                    normalized_date = (
                        f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                    )
                else:
                    normalized_date = f"{year}-{month.zfill(2)}"
                return normalized_date
            except Exception:
                continue

    return None


# ============================================
# è´¢ç»æ–°é—»æœç´¢ (å¢å¼ºç‰ˆ - å¤šç­–ç•¥å¬å›)
# ============================================


async def _execute_single_search(
    client,
    query: str,
    max_results: int,
    days: int = 7
) -> list:
    """æ‰§è¡Œå•æ¬¡æœç´¢å¹¶è¿”å›ç»“æœ"""
    try:
        response = client.search(
            query=query,
            search_depth="advanced",
            max_results=max_results,
            days=days,
            include_domains=list(SITE_CONFIG.keys()),
            include_raw_content=False,
            include_answer=False
        )
        return response.get("results", [])
    except Exception as e:
        print(f"[SEARCH] Query failed: {query[:50]}... - {e}")
        return []


def _group_results_by_topic(results: list) -> dict:
    """æŒ‰ä¸»é¢˜åˆ†ç»„ï¼šä¸šç»©ã€å…¬å‘Šã€ç ”æŠ¥ã€é‡å¤§äº‹é¡¹ã€å…¶ä»–"""
    groups = {
        "ä¸šç»©é¢„å‘Š": [],
        "å…¬å¸å…¬å‘Š": [],
        "ç ”æŠ¥è¯„çº§": [],
        "é‡å¤§äº‹é¡¹": [],
        "å…¶ä»–": []
    }

    for result in results:
        title = result.get("title", "").lower()

        # æŒ‰æ ‡é¢˜å…³é”®è¯åˆ†ç±»
        if any(kw in title for kw in
               ["ä¸šç»©", "é¢„å‘Š", "å¿«æŠ¥", "è´¢æŠ¥", "ä¸­æŠ¥", "å¹´æŠ¥"]):
            groups["ä¸šç»©é¢„å‘Š"].append(result)
        elif any(kw in title for kw in
                 ["å…¬å‘Š", "é€šçŸ¥", "è‚¡ä¸œå¤§ä¼š", "è‘£äº‹ä¼š"]):
            groups["å…¬å¸å…¬å‘Š"].append(result)
        elif any(kw in title for kw in
                 ["ç ”æŠ¥", "è¯„çº§", "ç›®æ ‡ä»·", "ä¹°å…¥", "å–å‡º", "ä¸­æ€§"]):
            groups["ç ”æŠ¥è¯„çº§"].append(result)
        elif any(kw in title for kw in
                 ["é‡ç»„", "å¹¶è´­", "åˆ†çº¢", "å®šå¢", "å›è´­", "åˆä½œ", "ç­¾çº¦"]):
            groups["é‡å¤§äº‹é¡¹"].append(result)
        else:
            groups["å…¶ä»–"].append(result)

    # ç§»é™¤ç©ºåˆ†ç»„
    return {k: v for k, v in groups.items() if v}


def _apply_source_priority_boost(results: list) -> list:
    """æ ¹æ®æ¥æºä¼˜å…ˆçº§è°ƒæ•´è´¨é‡åˆ†æ•°"""
    for result in results:
        url = result.get("url", "")
        base_score = result.get("score", 0.5)

        # ä»URLæå–åŸŸå
        for domain, priority in SOURCE_PRIORITIES.items():
            if domain in url:
                # åº”ç”¨ä¼˜å…ˆçº§åŠ æˆ (0.5~1.5å€)
                boosted_score = min(1.0, base_score * (0.8 + priority))
                result["score"] = boosted_score
                result["priority_boost"] = priority
                break

    return results


async def search_financial_news(
    symbol: str,
    stock_name: str,
    max_results: int = 10
) -> Dict:
    """
    æœç´¢æœ€æ–°çš„è´¢ç»æ–°é—»ã€ç ”æŠ¥å’Œå¸‚åœºæƒ…æŠ¥ (å¢å¼ºç‰ˆ - å¤šç­–ç•¥å¬å›)

    æ”¹è¿›ç‚¹:
    1. å¤šæ¬¡æœç´¢ç­–ç•¥: "æœ€æ–°æ¶ˆæ¯"ã€"ç ”æŠ¥è¯„çº§"ã€"ä¸šç»©é¢„å‘Š" 3ä¸ªæŸ¥è¯¢
    2. è´¨é‡è¿‡æ»¤: æ’é™¤çº¯ç´¢å¼•é¡µé¢ã€è¿‡çŸ­å†…å®¹
    3. å†…å®¹éªŒè¯: å¿…é¡»åŒ…å«è‚¡ç¥¨åç§°æˆ–ä»£ç 
    4. æ—¶æ•ˆæ€§æç¤º: æ ‡æ³¨"æœ€è¿‘7å¤©"è­¦å‘Š

    Args:
        symbol: è‚¡ç¥¨ä»£ç  (å¦‚ "688008")
        stock_name: è‚¡ç¥¨åç§° (å¦‚ "æ¾œèµ·ç§‘æŠ€")
        max_results: æœ€å¤§è¿”å›ç»“æœæ•°

    Returns:
        {
            "symbol": str,
            "stock_name": str,
            "query": str,
            "results": List[Dict],
            "summary": str,
            "search_time": str,
            "search_queries_used": list  # ä½¿ç”¨çš„æŸ¥è¯¢åˆ—è¡¨
        }
    """
    client = _get_tavily_client()
    if not client:
        return {
            "symbol": symbol,
            "stock_name": stock_name,
            "error": "Tavily not configured",
            "results": [],
            "summary": "ã€ç½‘ç»œæœç´¢æœªå¯ç”¨ã€‘è¯·è®¾ç½® TAVILY_API_KEY ç¯å¢ƒå˜é‡",
            "search_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "search_queries_used": []
        }

    # ====================================================================
    # ç­–ç•¥1: å¤šæŸ¥è¯¢å¹¶è¡Œå¬å› (æé«˜å¬å›ç‡) - 5è·¯æŸ¥è¯¢
    # ====================================================================
    search_queries = [
        # æŸ¥è¯¢1: æœ€æ–°æ¶ˆæ¯
        f'({stock_name} {symbol}) (æœ€æ–° æ¶ˆæ¯ åŠ¨æ€ å…¬å‘Š)',
        # æŸ¥è¯¢2: ç ”æŠ¥è¯„çº§
        f'({stock_name} {symbol}) (ç ”æŠ¥ è¯„çº§ ç›®æ ‡ä»· ä¹°å…¥ å–å‡º)',
        # æŸ¥è¯¢3: ä¸šç»©é¢„å‘Š
        f'({stock_name} {symbol}) (ä¸šç»© é¢„å‘Š è´¢æŠ¥ ä¸­æŠ¥ å¹´æŠ¥)',
        # æŸ¥è¯¢4: é‡å¤§äº‹é¡¹ (æ–°å¢)
        f'({stock_name} {symbol}) (é‡ç»„ å¹¶è´­ åˆ†çº¢ å®šå¢ å›è´­)',
        # æŸ¥è¯¢5: å…¬å¸å…¬å‘Š (æ–°å¢)
        f'({stock_name}) æŠ•èµ„è€…å…³ç³» æ´»åŠ¨ è·¯æ¼” è°ƒç ”'
    ]

    print(f"[SEARCH] Multi-strategy search for {symbol} - {stock_name}")
    print("[SEARCH] Query 1: æœ€æ–°æ¶ˆæ¯")
    print("[SEARCH] Query 2: ç ”æŠ¥è¯„çº§")
    print("[SEARCH] Query 3: ä¸šç»©é¢„å‘Š")
    print("[SEARCH] Query 4: é‡å¤§äº‹é¡¹")
    print("[SEARCH] Query 5: å…¬å¸å…¬å‘Š")

    all_results = []
    seen_urls = set()  # URLå»é‡
    seen_titles = set()  # æ ‡é¢˜å»é‡
    quality_threshold = 0.4  # åˆå§‹è´¨é‡é˜ˆå€¼
    days_window = 7  # åˆå§‹æ—¶é—´çª—

    try:
        # ç¬¬ä¸€è½®ï¼šæ‰§è¡Œ5ä¸ªæŸ¥è¯¢
        results_per_query = await asyncio.gather(
            _execute_single_search(
                client, search_queries[0], max_results, days_window
            ),
            _execute_single_search(
                client, search_queries[1], max_results, days_window
            ),
            _execute_single_search(
                client, search_queries[2], max_results, days_window
            ),
            _execute_single_search(
                client, search_queries[3], max_results, days_window
            ),
            _execute_single_search(
                client, search_queries[4], max_results, days_window
            )
        )

        # åˆå¹¶å¹¶å»é‡ç»“æœ
        for query_idx, query_results in enumerate(results_per_query):
            query_name = [
                "æœ€æ–°æ¶ˆæ¯", "ç ”æŠ¥è¯„çº§", "ä¸šç»©é¢„å‘Š",
                "é‡å¤§äº‹é¡¹", "å…¬å¸å…¬å‘Š"
            ][query_idx]
            print(
                f"[SEARCH] {query_name} query returned "
                f"{len(query_results)} results"
            )

            for result in query_results:
                url = result.get("url", "")
                title = result.get("title", "")
                content = result.get("content", "")

                # URLå»é‡
                if url in seen_urls:
                    continue
                seen_urls.add(url)

                # æ ‡é¢˜å»é‡ï¼ˆå»é™¤å®Œå…¨ç›¸åŒçš„æ ‡é¢˜ï¼‰
                title_normalized = title.strip().lower()
                if title_normalized in seen_titles:
                    continue
                seen_titles.add(title_normalized)

                # ====================================================================
                # ç­–ç•¥2: ä¸¥æ ¼è´¨é‡è¿‡æ»¤
                # ====================================================================

                # 2.1 æ’é™¤çº¯ç´¢å¼•é¡µé¢ï¼ˆåŒ…å«ç‰¹å¾å…³é”®è¯ï¼‰
                index_page_keywords = [
                    "æ•°æ®ä¸­å¿ƒ",
                    "æ•°æ®ç»Ÿè®¡",
                    "è¡Œæƒ…ä¸­å¿ƒ",
                    "f10æ•°æ®",
                    "ä¸ªè‚¡èµ„æ–™",
                    "è‚¡ç¥¨åˆ—è¡¨",
                    "å…¨éƒ¨è‚¡ç¥¨",
                    "æ•°æ®æŸ¥è¯¢",
                    "è¡Œæƒ…è½¯ä»¶",
                    "level1è¡Œæƒ…",
                    "ç›ˆäºé¢„æµ‹",
                    "ä¸šç»©é¢„å‘Šæ˜ç»†",
                    "ä¸šç»©é¢„å‘Šæ±‡æ€»è¡¨"
                ]
                is_index_page = any(kw in title for kw in index_page_keywords)
                if is_index_page:
                    print(f"[SEARCH] Filtered index page: {title[:40]}...")
                    continue

                # 2.2 å†…å®¹é•¿åº¦æ£€æŸ¥ï¼ˆè¿‡çŸ­å†…å®¹æ’é™¤ï¼‰
                if len(content) < 50:
                    print(f"[SEARCH] Filtered too short: {title[:40]}...")
                    continue

                # ====================================================================
                # ç­–ç•¥3: å†…å®¹ç›¸å…³æ€§éªŒè¯ (å¿…é¡»åŒ…å«è‚¡ç¥¨åç§°æˆ–ä»£ç )
                # ====================================================================
                # æ ‡é¢˜å’Œå†…å®¹ä¸­å¿…é¡»å‡ºç°è‚¡ç¥¨åç§°æˆ–ä»£ç 
                title_content = f"{title} {content}".lower()
                stock_name_lower = stock_name.lower()
                symbol_lower = symbol.lower()
                if (stock_name_lower not in title_content and
                        symbol_lower not in title_content):
                    print(f"[SEARCH] Filtered irrelevant: {title[:40]}...")
                    continue

                # ====================================================================
                # è´¨é‡è¯„åˆ†
                # ====================================================================
                quality_score = _calculate_content_quality_score(
                    title, content
                )

                # å™ªå£°æ£€æµ‹
                is_noise = _is_noise_content(title, content)
                if is_noise:
                    print(f"[SEARCH] Filtered noise: {title[:40]}...")
                    continue

                # åº”ç”¨è´¨é‡é˜ˆå€¼
                if quality_score >= quality_threshold:
                    all_results.append({
                        "title": title,
                        "url": url,
                        "content": content,
                        "score": quality_score,
                        "published_date": result.get("published_date", ""),
                        "is_realtime": False,
                        "query_source": query_name  # æ ‡è®°æ¥æºæŸ¥è¯¢
                    })
                else:
                    print(
                        f"[SEARCH] Low quality filtered: {title[:40]} "
                        f"(score: {quality_score:.2f})"
                    )

        # ====================================================================
        # ç­–ç•¥5: æ™ºèƒ½é™çº§æœºåˆ¶
        # ====================================================================
        # å¦‚æœç»“æœä¸è¶³3æ¡ï¼Œå¯åŠ¨é™çº§ç­–ç•¥
        if len(all_results) < 3:
            print(
                f"[SEARCH] [!] Results below threshold "
                f"({len(all_results)} < 3), activating fallback..."
            )

            # é™çº§1: æ‰©å±•æ—¶é—´çª—
            if days_window == 7:
                print("[SEARCH] Fallback 1: Expanding time window to 14 days")
                days_window = 14
                # é‡æ–°æ‰§è¡Œæ‰€æœ‰æŸ¥è¯¢
                fallback_results = await asyncio.gather(
                    _execute_single_search(
                        client, search_queries[0], max_results, days_window
                    ),
                    _execute_single_search(
                        client, search_queries[1], max_results, days_window
                    ),
                    _execute_single_search(
                        client, search_queries[2], max_results, days_window
                    ),
                    _execute_single_search(
                        client, search_queries[3], max_results, days_window
                    ),
                    _execute_single_search(
                        client, search_queries[4], max_results, days_window
                    )
                )

                # åˆå¹¶é™çº§ç»“æœ
                for query_idx, query_results in enumerate(fallback_results):
                    query_name = [
                        "æœ€æ–°æ¶ˆæ¯", "ç ”æŠ¥è¯„çº§", "ä¸šç»©é¢„å‘Š",
                        "é‡å¤§äº‹é¡¹", "å…¬å¸å…¬å‘Š"
                    ][query_idx]
                    for result in query_results:
                        url = result.get("url", "")
                        title = result.get("title", "")
                        content = result.get("content", "")

                        if url not in seen_urls:
                            seen_urls.add(url)
                            title_normalized = title.strip().lower()
                            if title_normalized not in seen_titles:
                                seen_titles.add(title_normalized)

                                # é‡å¤è´¨é‡æ£€æŸ¥
                                quality_score = (
                                    _calculate_content_quality_score(
                                        title, content
                                    )
                                )
                                if not _is_noise_content(
                                    title, content
                                ) and len(content) >= 50:
                                    title_content = (
                                        f"{title} {content}".lower()
                                    )
                                    condition = (
                                        stock_name_lower in title_content or
                                        symbol_lower in title_content
                                    )
                                    if condition:
                                        all_results.append({
                                            "title": title,
                                            "url": url,
                                            "content": content,
                                            "score": quality_score,
                                            "published_date": result.get(
                                                "published_date", ""
                                            ),
                                            "is_realtime": False,
                                            "query_source": (
                                                f"{query_name}(14å¤©)"
                                            )
                                        })

                print(f"[SEARCH] After fallback 1: {len(all_results)} results")

            # é™çº§2: é™ä½è´¨é‡é˜ˆå€¼ (å¦‚æœä»ç„¶ä¸è¶³)
            if len(all_results) < 3:
                print(
                    "[SEARCH] Fallback 2: Lowering quality threshold "
                    "to 0.3"
                )
                # æ·»åŠ è¢«ä½åˆ†è¿‡æ»¤çš„ç»“æœ
                for result in list(all_results):
                    if result.get("score", 0) < 0.4:
                        result["score"] += 0.15  # æå‡åˆ†æ•°
                        result["priority_boost"] = (
                            result.get("priority_boost", 0) + 0.1
                        )

        # ====================================================================
        # åº”ç”¨æ¥æºä¼˜å…ˆçº§åŠ æˆ
        # ====================================================================
        all_results = _apply_source_priority_boost(all_results)

        # æŒ‰è´¨é‡åˆ†æ•°æ’åºï¼ˆé«˜è´¨é‡ä¼˜å…ˆï¼‰
        sorted_results = sorted(
            all_results, key=lambda x: x.get("score", 0), reverse=True
        )

        # é™åˆ¶æœ€ç»ˆç»“æœæ•°é‡
        final_results = sorted_results[:max_results]

        # ====================================================================
        # ç»“æœåˆ†ç»„ç»Ÿè®¡
        # ====================================================================
        topic_groups = _group_results_by_topic(final_results)

        # æå–æ—¥æœŸå¹¶è§„èŒƒåŒ–
        has_published_date = False
        for result in final_results:
            content = result.get("content", "")
            extracted_date = _extract_and_normalize_date(content)
            if extracted_date:
                result["published_date"] = extracted_date
                has_published_date = True
            else:
                # ä½¿ç”¨Tavilyè¿”å›çš„æ—¥æœŸ
                tavily_date = result.get("published_date", "")
                if tavily_date:
                    result["published_date"] = tavily_date
                    has_published_date = True

        # ====================================================================
        # ç­–ç•¥4: æ—¶æ•ˆæ€§æç¤ºæ ‡æ³¨ + åˆ†ç»„å±•ç¤º
        # ====================================================================
        result_count = len(final_results)
        topic_info = "æ— åˆ†ç»„"
        summary = ""
        if result_count == 0:
            summary = (
                f"[ç½‘ç»œæƒ…æŠ¥ - {stock_name}]\\n[!] æœªæ‰¾åˆ°ç›¸å…³æ–°é—»ã€‚"
                "å¯èƒ½åŸå› ï¼š1) Tavilyæ•°æ®åº“è¦†ç›–ä¸è¶³ 2) æœç´¢æ—¶é—´çª—è¿‡çª„ "
                "3) è¯¥è‚¡ç¥¨è¿‘æœŸæ— é‡å¤§äº‹ä»¶"
            )
        elif result_count < 3:
            summary = (
                f"[ç½‘ç»œæƒ…æŠ¥ - {stock_name} (æœ€è¿‘{days_window}å¤©)]\\n[!] "
                f"ä»…æ‰¾åˆ° {result_count} æ¡ç»“æœï¼ŒTavilyæ•°æ®è¦†ç›–æœ‰é™ã€‚"
                "å·²å¯ç”¨æ™ºèƒ½é™çº§(æ‰©å±•æ—¶é—´çª—)ï¼Œå»ºè®®æŸ¥é˜…å…¬å¸å®˜ç½‘æˆ–äº¤æ˜“æ‰€"
                "å…¬å‘Šè·å–æœ€æ–°ä¿¡æ¯ã€‚"
            )
        elif not has_published_date:
            summary = (
                f"[ç½‘ç»œæƒ…æŠ¥ - {stock_name} (æœ€è¿‘{days_window}å¤©)]\\n"
                f"æ‰¾åˆ° {result_count} æ¡ç›¸å…³ç»“æœã€‚\\n[!] è­¦å‘Š: "
                "Tavilyæœªè¿”å›å‘å¸ƒæ—¥æœŸï¼Œæ—¶æ•ˆæ€§éœ€äººå·¥éªŒè¯ã€‚"
            )
        else:
            # æ˜¾ç¤ºç»“æœæ¥æºåˆ†å¸ƒ
            source_counts = {}
            for r in final_results:
                qs = r.get("query_source", "unknown")
                source_counts[qs] = source_counts.get(qs, 0) + 1
            source_info = ", ".join(
                [f"{k}:{v}" for k, v in source_counts.items()]
            )

            # æ˜¾ç¤ºä¸»é¢˜åˆ†ç»„
            topic_summary = []
            for topic, items in topic_groups.items():
                if items:
                    topic_summary.append(f'{topic}({len(items)})')
            topic_info = " | ".join(topic_summary) if topic_summary else "æ— åˆ†ç»„"

            summary = (
                f"ã€ç½‘ç»œæƒ…æŠ¥ - {stock_name} (æœ€è¿‘{days_window}å¤©)ã€‘\n"
                f"æ‰¾åˆ° {result_count} æ¡ç›¸å…³ç»“æœ ({source_info})ï¼Œ"
                f"ä¸»é¢˜åˆ†å¸ƒ: {topic_info}ï¼Œå·²å±•ç¤ºè´¨é‡æœ€é«˜çš„ "
                f"{min(5, result_count)} æ¡ã€‚"
            )

        print(
            f"[SEARCH] Final results: {result_count} "
            f"(from {len(all_results)} total)"
        )
        if result_count > 0:
            print(f"[SEARCH] Topic breakdown: {topic_info}")

        return {
            "symbol": symbol,
            "stock_name": stock_name,
            "query": " + ".join(search_queries),
            "results": final_results,
            "summary": summary,
            "search_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "search_queries_used": ["æœ€æ–°æ¶ˆæ¯", "ç ”æŠ¥è¯„çº§", "ä¸šç»©é¢„å‘Š", "é‡å¤§äº‹é¡¹", "å…¬å¸å…¬å‘Š"],
            "total_fetched": len(all_results),
            "has_published_date": has_published_date,
            "topic_groups": topic_groups,
            "days_window_used": days_window,
            "quality_threshold_used": quality_threshold
        }

    except Exception as e:
        print(f"[SEARCH] ERROR: {e}")
        import traceback
        traceback.print_exc()
        return {
            "symbol": symbol,
            "stock_name": stock_name,
            "error": str(e),
            "results": [],
            "summary": f"ã€ç½‘ç»œæœç´¢å¼‚å¸¸ã€‘{str(e)}",
            "search_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "search_queries_used": ["æœ€æ–°æ¶ˆæ¯", "ç ”æŠ¥è¯„çº§", "ä¸šç»©é¢„å‘Š", "é‡å¤§äº‹é¡¹", "å…¬å¸å…¬å‘Š"],
            "total_fetched": 0,
            "has_published_date": False,
            "topic_groups": {},
            "days_window_used": 7,
            "quality_threshold_used": 0.4
        }


# ============================================
# å…¬å¸ä¿¡æ¯æœç´¢
# ============================================

async def search_company_info(symbol: str, stock_name: str) -> Dict:
    """
    æœç´¢å…¬å¸åŸºæœ¬ä¿¡æ¯ã€ä¸šåŠ¡æè¿°ï¼ˆä½œä¸º Tushare çš„è¡¥å……ï¼‰

    Args:
        symbol: è‚¡ç¥¨ä»£ç 
        stock_name: è‚¡ç¥¨åç§°

    Returns:
        {
            "company_info": str,
            "main_business": str,
            "industry_info": str
        }
    """
    client = _get_tavily_client()
    if not client:
        return {
            "company_info": "ã€ç½‘ç»œæœç´¢æœªå¯ç”¨ã€‘è¯·è®¾ç½® TAVILY_API_KEY ç¯å¢ƒå˜é‡",
            "main_business": "",
            "industry_info": ""
        }

    # é¢„å¤„ç†æŸ¥è¯¢
    processed_query = _preprocess_query(symbol, stock_name, "company")

    print(f"[SEARCH] Tavily searching company: {processed_query}")

    try:
        response = client.search(
            query=processed_query,
            search_depth="basic",  # å…¬å¸ä¿¡æ¯ç”¨åŸºç¡€æœç´¢å³å¯
            max_results=3,
            days=30  # å…¬å¸ä¿¡æ¯ç›¸å¯¹ç¨³å®šï¼Œå¯ä»¥æŸ¥æ›´é•¿æ—¶é—´çª—
        )

        if not response or "results" not in response:
            print(
                "[SEARCH] WARNING: Tavily returned invalid company "
                "info response"
            )
            return {
                "company_info": "ã€ç½‘ç»œæœç´¢å¼‚å¸¸ã€‘Tavilyè¿”å›äº†æ— æ•ˆå“åº”",
                "main_business": "",
                "industry_info": ""
            }

        # è§£æç»“æœ
        main_business_parts = []
        industry_info_parts = []

        for result in response.get("results", []):
            title = result.get("title", "")
            content = result.get("content", "")

            # åˆ†ç±»æå–
            if any(kw in title for kw in ["ä¸»è¥", "ä¸šåŠ¡", "äº§å“"]):
                main_business_parts.append(f"{title}: {content}")
            elif any(kw in title for kw in ["è¡Œä¸š", "æ¿å—", "æ‰€å±"]):
                industry_info_parts.append(f"{title}: {content}")

        # æ„å»ºè¿”å›ç»“æœ
        if main_business_parts or industry_info_parts:
            company_info = (
                "\n".join(main_business_parts) if main_business_parts
                else "æœªæ‰¾åˆ°å…¬å¸ä¸»è¥ä¸šåŠ¡ä¿¡æ¯"
            )
            industry_info = (
                "\n".join(industry_info_parts) if industry_info_parts
                else "æœªæ‰¾åˆ°è¡Œä¸šä¿¡æ¯"
            )
        else:
            company_info = "æœªæ‰¾åˆ°å…¬å¸ä¸»è¥ä¸šåŠ¡ä¿¡æ¯"
            industry_info = "æœªæ‰¾åˆ°è¡Œä¸šä¿¡æ¯"

        results_count = len(response.get('results', []))
        print(f"[SEARCH] Tavily found {results_count} company info results")

        return {
            "company_info": f"{company_info}\n{industry_info}",
            "main_business": company_info,
            "industry_info": industry_info
        }

    except Exception as e:
        print(f"[SEARCH] ERROR: {e}")
        import traceback
        traceback.print_exc()
        return {
            "company_info": f"ã€ç½‘ç»œæœç´¢å¼‚å¸¸ã€‘{str(e)}",
            "main_business": "",
            "industry_info": ""
        }


# ============================================
# æ ¼å¼åŒ–æœç´¢ç»“æœä¸º LLM ä¸Šä¸‹æ–‡
# ============================================

def format_search_context_for_llm(search_result: Dict, stock_name: str) -> str:
    """
    å°†æœç´¢ç»“æœæ ¼å¼åŒ–ä¸º LLM å¯è¯»çš„ä¸Šä¸‹æ–‡ (å¢å¼ºç‰ˆ - æ”¯æŒä¸»é¢˜åˆ†ç»„)

    Args:
        search_result: search_financial_news çš„è¿”å›å€¼
        stock_name: è‚¡ç¥¨åç§°

    Returns:
        æ ¼å¼åŒ–çš„æ–‡æœ¬ä¸Šä¸‹æ–‡
    """
    if not search_result or search_result.get("error"):
        return "\nã€ç½‘ç»œæƒ…æŠ¥ã€‘ç½‘ç»œæœç´¢ä¸å¯ç”¨ï¼Œä¾èµ–å·²æœ‰æ•°æ®ã€‚\n"

    results = search_result.get("results", [])
    if not results:
        default_msg = f'æœªæ‰¾åˆ° {stock_name} çš„æœ€æ–°æ–°é—»ã€‚'
        summary = search_result.get('summary', default_msg)
        return f"\nã€ç½‘ç»œæƒ…æŠ¥ã€‘{summary}\n"

    # æŒ‰è´¨é‡åˆ†æ•°æ’åº
    sorted_results = sorted(
        results,
        key=lambda x: x.get("score", 0),
        reverse=True
    )

    # æ„å»ºä¸Šä¸‹æ–‡ï¼ˆæŒ‰è´¨é‡é™åºï¼‰
    context = f"\nã€ç½‘ç»œæƒ…æŠ¥ - {stock_name} æœ€æ–°åŠ¨æ€ã€‘\n"
    context += f"æœç´¢æ—¶é—´: {search_result.get('search_time', '')}\n"

    # æ˜¾ç¤ºæŸ¥è¯¢ç­–ç•¥ç»Ÿè®¡
    queries_used = search_result.get("search_queries_used", [])
    if queries_used:
        context += f"æŸ¥è¯¢ç­–ç•¥: {', '.join(queries_used)} (5è·¯å¹¶è¡Œ)\n"
    total_fetched = search_result.get("total_fetched", len(results))
    context += f"å¬å›ç»Ÿè®¡: å…±æ£€ç´¢ {total_fetched} æ¡ï¼Œå»é‡åä¿ç•™ {len(results)} æ¡\n"

    # æ˜¾ç¤ºæ™ºèƒ½é™çº§ä¿¡æ¯
    days_window = search_result.get("days_window_used", 7)
    if days_window > 7:
        context += f"[!] æ™ºèƒ½é™çº§: æ—¶é—´çª—æ‰©å±•è‡³ {days_window} å¤©\\n"
    quality_threshold = search_result.get("quality_threshold_used", 0.4)
    context += f"è´¨é‡é˜ˆå€¼: {quality_threshold}\n"

    has_date = search_result.get("has_published_date", True)
    if not has_date:
        context += "[!] è­¦å‘Š: Tavilyæœªè¿”å›å‘å¸ƒæ—¥æœŸï¼Œæ—¶æ•ˆæ€§éœ€äººå·¥éªŒè¯\\n"

    context += "=" * 50 + "\n"

    # æŒ‰ä¸»é¢˜åˆ†ç»„å±•ç¤º
    topic_groups = search_result.get("topic_groups", {})
    if topic_groups and any(topic_groups.values()):
        context += "ã€ä¸»é¢˜åˆ†ç»„ã€‘\n"
        for topic, items in topic_groups.items():
            if items:
                context += f"  [Topic] {topic}: {len(items)} æ¡\\n"
                # æ¯ä¸ªä¸»é¢˜æœ€å¤šæ˜¾ç¤º2æ¡
                for item in items[:2]:
                    context += f"     â€¢ {item.get('title', '')[:50]}\n"
        context += "\n"

    # é€æ¡å±•ç¤ºæ–°é—»ï¼ˆåªæ˜¾ç¤ºå‰5æ¡é«˜è´¨é‡ç»“æœï¼‰
    context += "\nã€è¯¦ç»†ç»“æœã€‘\n"
    for i, result in enumerate(sorted_results[:5], 1):
        title = result.get("title", "")
        score = result.get("score", 0)
        date_str = result.get("published_date", "")
        query_source = result.get("query_source", "")
        priority_boost = result.get("priority_boost", 0)
        is_realtime = not result.get("is_realtime", True)

        # æ ‡è®°éå®æ—¶ä¿¡æ¯
        realtime_label = "" if is_realtime else " [å†å²]"

        # æ¥æºæ ‡ç­¾å’Œä¼˜å…ˆçº§æ ‡è®°
        source_label = f"[{query_source}]" if query_source else ""
        if priority_boost and priority_boost >= 0.8:
            priority_label = "ğŸ”¼"
        elif priority_boost and priority_boost >= 0.6:
            priority_label = "ğŸ“Š"
        else:
            priority_label = ""

        context += f"{i}. ã€{title}ã€‘{source_label}{realtime_label}\n"
        context += f"   æ¥æº: {result.get('url', '')}\n"
        context += f"   å‘å¸ƒæ—¶é—´: {date_str if date_str else 'æœªçŸ¥'}\n"
        context += f"   å†…å®¹è´¨é‡: {score:.2f}/1.0 {priority_label}\n"
        context += "-" * 40 + "\n"

        # å¦‚æœæœ‰æ›´å¤šç»“æœï¼Œæ·»åŠ æç¤º
        if i == 4 and len(sorted_results) > 5:
            context += (
                f"   (è¿˜æœ‰ {len(sorted_results) - 5} æ¡ç»“æœå·²è¿‡æ»¤ï¼Œ"
                "å¯é€šè¿‡æ‰©å¤§æ—¶é—´çª—æŸ¥çœ‹)"
            )

        context += (
            "=[!] é£é™©æç¤º[!]\\nä»¥ä¸Šä¿¡æ¯æ¥è‡ªç½‘ç»œæœç´¢ï¼Œ"
            "è¯·ç»“åˆå…¬å¸å®˜æ–¹æŠ«éœ²ä¿¡æ¯ç»¼åˆåˆ¤æ–­ã€‚"
            "éƒ¨åˆ†å†…å®¹å¯èƒ½å­˜åœ¨æ—¶æ•ˆæ€§æ»åæˆ–å‡†ç¡®æ€§é—®é¢˜ï¼Œ"
            "å»ºè®®æŸ¥é˜…å…¬å¸æœ€æ–°å…¬å‘Šã€‚\\n"
        )

    # è¿”å›ç»“æ„åŒ–æ•°æ®ï¼ˆJSON æ ¼å¼ï¼‰ä¾› IC æŠ•å§”ä¼šå¤„ç†
    structured_data = {
        "tavily_data": {
            "results": results,
            "total_fetched": search_result.get(
                "total_fetched", len(results)
            ),
            "search_time": search_result.get("search_time", ""),
            "quality_threshold": search_result.get(
                "quality_threshold_used", 0.4
            ),
            "summary": search_result.get("summary", "")
        }
    }
    import json
    return json.dumps(structured_data, ensure_ascii=False)


# ============================================
# ä¸»å‡½æ•° - åŒæ­¥ç‰ˆæœ¬
# ============================================


async def search_financial_news_sync(
    symbol: str,
    stock_name: str,
    max_results: int = 5
) -> Dict:
    """åŒæ­¥ç‰ˆæœ¬çš„è´¢ç»æ–°é—»æœç´¢"""
    return await asyncio.run(
        search_financial_news(symbol, stock_name, max_results)
    )


async def search_company_info_sync(symbol: str, stock_name: str) -> Dict:
    """åŒæ­¥ç‰ˆæœ¬çš„å…¬å¸ä¿¡æ¯æœç´¢"""
    return await asyncio.run(search_company_info(symbol, stock_name))


# ============================================
# ä¸»å‡½æ•° - ç”¨äºéå¼‚æ­¥ç¯å¢ƒ
# ============================================

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    # result = asyncio.run(search_financial_news("688008", "æ¾œèµ·ç§‘æŠ€", 5))
    # print(format_search_context_for_llm(result, "æ¾œèµ·ç§‘æŠ€"))
    # æµ‹è¯•å…¬å¸æœç´¢
    # result = asyncio.run(search_company_info("688008", "æ¾œèµ·ç§‘æŠ€"))

    print("[SEARCH] Search service starting...")
    print("[SEARCH] Current configuration:")
    print(f"[SEARCH]   - Max results per query: {5}")
    print("[SEARCH]   - Time window: 7 days")
    print("[SEARCH]   - Content de-noising: ENABLED")
    print("[SEARCH]   - Advanced search: ENABLED")
    print("[SEARCH]   - Raw content: DISABLED")
    print("[SEARCH]   - Result layering: ENABLED")
    print("[SEARCH]   - Date normalization: ENABLED")
