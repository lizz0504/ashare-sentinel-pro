# Aè‚¡Sentinel Pro - ICæŠ•å§”ä¼šå®Œæ•´ä»£ç æ–‡æ¡£

**ç”Ÿæˆæ—¶é—´**: 2026-02-16
**ç›®æ ‡**: åœ¨ä¸æ”¹å˜é€»è¾‘çš„æƒ…å†µä¸‹ï¼Œå°†ICæŠ•å§”ä¼šåˆ†ææ—¶é—´ä»10-20åˆ†é’Ÿå‹ç¼©åˆ°60ç§’

---

## ğŸ“‹ ç›®å½•

1. [é—®é¢˜æè¿°](#é—®é¢˜æè¿°)
2. [æ ¸å¿ƒä»£ç  - conduct_meetingå‡½æ•°](#æ ¸å¿ƒä»£ç ---conduct_meetingå‡½æ•°)
3. [LLMè°ƒç”¨å·¥å‚](#llmè°ƒç”¨å·¥å‚)
4. [AIæç¤ºè¯å®šä¹‰](#aiæç¤ºè¯å®šä¹‰)
5. [æ€§èƒ½ç“¶é¢ˆåˆ†æ](#æ€§èƒ½ç“¶é¢ˆåˆ†æ)
6. [ä¼˜åŒ–å»ºè®®](#ä¼˜åŒ–å»ºè®®)

---

## é—®é¢˜æè¿°

### å½“å‰çŠ¶æ€
- **æ­£å¸¸æµç¨‹**: åº”è¯¥åœ¨60-90ç§’å†…å®Œæˆ
- **å®é™…çŠ¶æ€**: éœ€è¦10-20åˆ†é’Ÿ
- **ç—‡çŠ¶**: åç«¯å‡ºç°58æ­¥éª¤tqdmè¿›åº¦æ¡ï¼ˆå·²ç¦ç”¨Tavilyæœç´¢åæ¶ˆå¤±ï¼‰

### ICæŠ•å§”ä¼šæ¶æ„

```
Round 1: å¹¶è¡Œæ‰§è¡Œ (Cathie Wood + Nancy Pelosi) â†’ 2ä¸ªLLMè°ƒç”¨
Round 2: é¡ºåºæ‰§è¡Œ (Warren Buffett) â†’ 1ä¸ªLLMè°ƒç”¨
Round 3: æœ€ç»ˆå†³ç­– (Charlie Munger) â†’ 1ä¸ªLLMè°ƒç”¨

æ€»è®¡: 4æ¬¡LLMè°ƒç”¨
```

---

## æ ¸å¿ƒä»£ç  - conduct_meetingå‡½æ•°

**æ–‡ä»¶**: `backend/app/services/ic_service.py` (ç¬¬413-896è¡Œ)

```python
async def conduct_meeting(
    symbol: str,
    stock_name: str,
    current_price: float,
    context: Optional[Dict[str, Any]] = None,
    api_key: Optional[str] = None
) -> Dict[str, Any]:
    """
    Conduct an AI Investment Committee meeting for a given stock.

    Meeting Flow:
    1. Parallel: Cathie Wood + Nancy Pelosi analyze independently
    2. Sequential: Warren Buffett reviews Cathie + Nancy's views
    3. Final: Charlie Munger synthesizes all views into JSON verdict
    """
    logger.info(f"Starting IC meeting for {symbol} ({stock_name})")

    # Prepare context
    if context is None:
        context = {}
        # è·å–è´¢åŠ¡æ•°æ®
        from app.services.market_service import calculate_financial_metrics
        financial_metrics = calculate_financial_metrics(symbol)
        context.update(financial_metrics.get('metrics', {}))
        context['market'] = financial_metrics.get('market', 'A')
        context['symbol'] = financial_metrics.get('symbol', symbol)

    base_context = f"""
Stock: {symbol} - {stock_name}
Current Price: {current_price}
Industry: {context.get('industry', 'N/A')}
Market Cap: {context.get('market_cap', 'N/A')}

=== Value Metrics (Warren Buffett) ===
PE Ratio: {context.get('pe_ratio', 'N/A')}
PB Ratio: {context.get('pb_ratio', 'N/A')}
ROE: {context.get('roe', 'N/A')}
Debt-to-Equity: {context.get('debt_to_equity', 'N/A')}
FCF Yield: {context.get('fcf_yield', 'N/A')}

=== Growth Metrics (Cathie Wood) ===
Revenue Growth (CAGR): {context.get('revenue_growth_cagr', 'N/A')}
PEG Ratio: {context.get('peg_ratio', 'N/A')}
R&D Intensity: {context.get('rd_intensity', 'N/A')}

=== Technical & Momentum Metrics (Nancy Pelosi) ===
RSI (14): {context.get('rsi_14', 'N/A')}
Volume Status: {context.get('volume_status', 'N/A')}
Volume Change %: {context.get('volume_change_pct', 'N/A')}%
Turnover Rate: {context.get('turnover_rate', 'N/A')}%
MA20 Status: {context.get('ma20_status', 'N/A')}
Health Score: {context.get('health_score', 'N/A')}/100
Action Signal: {context.get('action_signal', 'N/A')}
Bollinger Band Width: {context.get('bb_width', 'N/A')}
VWAP (20-day): {context.get('vwap_20d', 'N/A')}
Bollinger Position: {context.get('bollinger_position', 'N/A')}
"""

    # ========================================================================
    # NEW: Enhanced Context Injection (Anti-Hallucination Data)
    # ========================================================================
    injection_context = ""
    profile_data = None
    news_result = None

    try:
        # 1. Fetch Tushare Profile (Official Company Identity)
        logger.info(f"[ENHANCED] Fetching Tushare profile for {symbol}...")
        from app.services.market_service import get_stock_main_business_tushare

        try:
            profile_data = get_stock_main_business_tushare(symbol)
            logger.info("[ENHANCED] Tushare profile fetched successfully")

            # è·å–å½“å‰ä»·æ ¼
            from app.services.data_fetcher import DataFetcher
            fetcher = DataFetcher()
            stock_info = fetcher.get_stock_info(symbol)
            current_price = stock_info.get("current_price", 0) if stock_info else 0
        except Exception as tushare_error:
            logger.warning(f"[ENHANCED] Tushare fetch failed: {tushare_error}")
            profile_data = None

        # 2. Fetch Tavily Intelligence (Real-time News Intelligence) - DISABLED
        # from app.services.search_service import search_financial_news, format_search_context_for_llm
        logger.info(f"[ENHANCED] Skipping Tavily search to avoid 58-step progress bar issue")
        news_result = {"results": [], "summary": "ç½‘ç»œæœç´¢å·²ç¦ç”¨ä»¥ä¼˜åŒ–æ€§èƒ½"}

        # è§£æ Tavily JSON æ•°æ®
        tavily_context_json = format_search_context_for_llm(news_result, stock_name)
        try:
            tavily_data = json.loads(tavily_context_json)
            tavily_structured = tavily_data.get("tavily_data", {})
            tavily_summary = tavily_data.get("summary", "")
            tavily_results = tavily_structured.get("results", [])
            tavily_total = tavily_structured.get("total_fetched", 0)
        except Exception as e:
            logger.warning(f"[ENHANCED] Failed to parse Tavily JSON: {e}")
            tavily_summary = tavily_context_json

        # 3. Build Injection Payload
        if profile_data or news_result.get('results'):
            injection_context = """
ã€=== æ–°å¢é«˜ç»´æ•°æ®è¾“å…¥ (Anti-Hallucination) ===ã€‘
"""
            if profile_data:
                injection_context += f"""
1. å…¬å¸å®˜æ–¹èº«ä»½ (Tushare Profile):
   - è‚¡ç¥¨ä»£ç : {profile_data.get('symbol', 'N/A')}
   - è‚¡ç¥¨åç§°: {profile_data.get('name', 'N/A')}
   - æ‰€å±è¡Œä¸š: {profile_data.get('industry', 'N/A')}
   - æ‰€åœ¨åœ°: {profile_data.get('area', 'N/A')}
   - ä¸»è¥ä¸šåŠ¡: {profile_data.get('main_business', 'N/A')}
   - ç»è¥èŒƒå›´: {profile_data.get('business_scope', 'N/A')[:200]}...
"""

            if news_result.get('results'):
                injection_context += f"""
2. å¸‚åœºå®æ—¶æƒ…æŠ¥ (Tavily Search):
{tavily_summary}
"""

            injection_context += """
ã€é‡è¦ã€‘è¯·åŸºäºä»¥ä¸Šæ–°å¢äº‹å®æ•°æ®ï¼Œç»“åˆä½ åŸæœ‰çš„æŠ•èµ„é€»è¾‘è¿›è¡Œåˆ†æã€‚
"""
            logger.info(f"[ENHANCED] Injected Tushare Profile + Tavily Intel for {symbol}")

    except Exception as e:
        logger.warning(f"[ENHANCED] Failed to fetch anti-hallucination data: {e}")
        injection_context = ""

    # Merge injection context into base context
    enhanced_base_context = base_context + injection_context

    # ========================================================================
    # Round 1: Parallel Execution (Cathie + Nancy)
    # ========================================================================
    logger.info("Round 1: Parallel execution - Cathie Wood + Nancy Pelosi")

    try:
        # Create tasks for parallel execution
        cathie_task = call_llm_async(
            f"{PROMPT_CATHIE_WOOD}\n\n{enhanced_base_context}",
            api_key or ""
        )

        nancy_task = call_llm_async(
            f"{PROMPT_NANCY_PELOSI}\n\n{enhanced_base_context}",
            api_key or ""
        )

        # Execute in parallel
        cathie_response, nancy_response = await asyncio.gather(
            cathie_task,
            nancy_task
        )

        logger.info("Round 1 completed: Received responses from Cathie and Nancy")

    except Exception as e:
        logger.error(f"Round 1 failed: {str(e)}")
        cathie_response = f"Error: Cathie Wood analysis failed - {str(e)}"
        nancy_response = f"Error: Nancy Pelosi analysis failed - {str(e)}"

    # ========================================================================
    # Round 2: Sequential Execution (Warren Buffett)
    # ========================================================================
    logger.info("Round 2: Sequential execution - Warren Buffett")

    # æˆªæ–­å‰ä¸¤è½®å“åº”
    cathie_summary = truncate_with_summary(cathie_response, 200)
    nancy_summary = truncate_with_summary(nancy_response, 200)

    warren_context = f"""
{enhanced_base_context}

## Previous Analysts' Views (Summarized)

### Cathie Wood (Growth Perspective):
{cathie_summary}

### Nancy Pelosi (Policy Perspective):
{nancy_summary}

## Your Task
Review the summarized perspectives above, then provide your value investing analysis.
**Please be concise** - limit your response to 300 words maximum.
"""

    try:
        warren_response = await call_llm_async(
            f"{PROMPT_WARREN_BUFFETT}\n\n{warren_context}",
            api_key or ""
        )
        logger.info("Round 2 completed: Received response from Warren Buffett")

    except Exception as e:
        logger.error(f"Round 2 failed: {str(e)}")
        warren_response = f"Error: Warren Buffett analysis failed - {str(e)}"

    # ========================================================================
    # Round 3: Final Verdict (Charlie Munger)
    # ========================================================================
    logger.info("Round 3: Final verdict - Charlie Munger")

    # è®¡ç®—å¯ç”¨ token é¢„ç®—
    prompt_tokens = estimate_tokens(PROMPT_CHARLIE_MUNGER)
    context_tokens = estimate_tokens(base_context)
    available_for_summaries = (
        MAX_PROMPT_TOKENS - prompt_tokens - context_tokens - 5000
    )

    if available_for_summaries < 1000:
        logger.error(f"Not enough token budget for Charlie: {available_for_summaries}")
        available_for_summaries = 1000

    summary_limit = available_for_summaries // 3

    # ä½¿ç”¨ token æ„ŸçŸ¥çš„æˆªæ–­
    cathie_brief = truncate_text_by_tokens(cathie_response, summary_limit)
    nancy_brief = truncate_text_by_tokens(nancy_response, summary_limit)
    warren_brief = truncate_text_by_tokens(warren_response, summary_limit)

    # è¿›ä¸€æ­¥å‹ç¼©ä¸ºæç®€æ‘˜è¦
    cathie_brief = truncate_with_summary(cathie_brief, 150)
    nancy_brief = truncate_with_summary(nancy_brief, 150)
    warren_brief = truncate_with_summary(warren_brief, 150)

    charlie_context = f"""
{enhanced_base_context}

## IC Meeting Summary (Brief)

### Cathie Wood (Growth & Disruption):
{cathie_brief}

### Nancy Pelosi (Power & Policy):
{nancy_brief}

### Warren Buffett (Deep Value):
{warren_brief}

## Your Task
Review the summarized perspectives above, then provide your FINAL VERDICT in JSON format.

**CRITICAL OUTPUT REQUIREMENTS:**
- Do NOT output markdown formatting (no ```json or ``` blocks)
- Output RAW JSON only
- Keep the "synthesis" field under 50 words
- Keep each "key_consideration" under 20 words
- Keep each "invert_risk" under 15 words
"""

    try:
        charlie_response = await call_llm_async(
            f"{PROMPT_CHARLIE_MUNGER}\n\n{charlie_context}",
            api_key or ""
        )
        logger.info("Round 3 completed: Received response from Charlie Munger")

        # Parse Charlie's JSON response
        final_verdict = clean_and_parse_json(charlie_response)

    except Exception as e:
        logger.error(f"Round 3 failed: {str(e)}")
        final_verdict = {
            "final_verdict": "HOLD",
            "conviction_level": 3,
            "key_considerations": [f"AIåˆ†æå¤±è´¥: {str(e)}"],
            "invert_risks": ["æŠ€æœ¯æ•…éšœé£é™©"],
            "synthesis": "ç”±äºæŠ€æœ¯åŸå› ï¼Œæ— æ³•å®ŒæˆæŠ•å§”ä¼šä¼šè®®ã€‚"
        }

    # ========================================================================
    # Compile Results
    # ========================================================================
    verdict_key = "decision" if "decision" in final_verdict else "final_verdict"
    conviction_key = "conviction" if "conviction" in final_verdict else "conviction_level"
    risks_key = "risk_factors" if "risk_factors" in final_verdict else "invert_risks"

    normalized_verdict = {
        "final_verdict": final_verdict.get(
            verdict_key, final_verdict.get("final_verdict", "HOLD")
        ),
        "conviction_level": final_verdict.get(
            conviction_key, final_verdict.get("conviction_level", 3)
        ),
        "key_considerations": final_verdict.get("key_considerations", []),
        "invert_risks": final_verdict.get(risks_key, final_verdict.get("invert_risks", [])),
        "synthesis": final_verdict.get("synthesis", ""),
        "score": final_verdict.get("score"),
        "logical_flaws_detected": final_verdict.get("logical_flaws_detected", []),
    }

    verdict_chinese = VERDICT_MAP.get(normalized_verdict["final_verdict"], "æŒæœ‰")
    conviction_level = normalized_verdict["conviction_level"]
    conviction_stars = CONVICTION_LEVELS.get(conviction_level, "***")

    # è®¡ç®—æŠ€æœ¯é¢å’ŒåŸºæœ¬é¢å¾—åˆ†
    technical_score = calculate_technical_score(context)
    fundamental_score = calculate_fundamental_score(context)

    # æå–è§’è‰²è¯„åˆ†
    cathie_score_data = extract_agent_score(cathie_response)
    nancy_score_data = extract_agent_score(nancy_response)
    warren_score_data = extract_agent_score(warren_response)

    # è®¡ç®—Dashboardåæ ‡
    fundamental_x = int((warren_score_data["score"] * 0.6) + (nancy_score_data["score"] * 0.4))
    fundamental_x = max(0, min(100, fundamental_x))

    trend_y = int((cathie_score_data["score"] * 0.5) + (technical_score * 0.5))
    trend_y = max(0, min(100, trend_y))

    result = {
        "symbol": symbol,
        "stock_name": stock_name,
        "current_price": current_price,
        "cathie_wood": cathie_response,
        "nancy_pelosi": nancy_response,
        "warren_buffett": warren_response,
        "charlie_munger_raw": charlie_response if 'charlie_response' in locals() else "Error",
        "final_verdict": normalized_verdict,
        "verdict_chinese": verdict_chinese,
        "conviction_level": conviction_level,
        "conviction_stars": conviction_stars,
        "technical_score": technical_score,
        "fundamental_score": fundamental_score,
        "timestamp": context.get("timestamp", ""),
        "agent_scores": {
            "cathie_wood": cathie_score_data,
            "nancy_pelosi": nancy_score_data,
            "warren_buffett": warren_score_data
        },
        "dashboard_position": {
            "final_x": fundamental_x,
            "final_y": trend_y
        }
    }

    logger.info(f"IC meeting completed for {symbol}: {verdict_chinese} {conviction_stars}")
    return result
```

---

## LLMè°ƒç”¨å·¥å‚

**æ–‡ä»¶**: `backend/app/core/llm_factory.py`

```python
class LLMFactory:
    """å¤šæ¨¡å‹AIè°ƒç”¨å·¥å‚"""

    APIS = {
        "deepseek": "https://api.deepseek.com/v1/chat/completions",
        "zhipu": "https://open.bigmodel.cn/api/paas/v4/chat/completions"
    }

    MODELS = {
        "deepseek": "deepseek-chat",
        "zhipu": "glm-4"
    }

    NAMES = {
        "deepseek": "DeepSeek",
        "zhipu": "æ™ºè°±GLM"
    }

    @classmethod
    async def fast_reply(
        cls,
        model: str,
        system: str,
        user: str,
        timeout: int = 60  # å¢åŠ åˆ° 60 ç§’ï¼ˆåŸ 30 ç§’ï¼‰
    ) -> str:
        """å¿«é€Ÿè°ƒç”¨æ¨¡å‹"""
        caller = {
            "deepseek": cls._call_deepseek,
            "zhipu": cls._call_zhipu
        }.get(model)

        if caller:
            return await caller(system, user, timeout)
        return f"[é”™è¯¯] æœªçŸ¥æ¨¡å‹: {model}"

    @classmethod
    async def _call_deepseek(cls, system: str, user: str, timeout: int) -> str:
        """è°ƒç”¨DeepSeek"""
        api_key = getattr(settings, 'DEEPSEEK_API_KEY', None)
        if not api_key:
            logger.warning("DeepSeekæœªé…ç½®ï¼Œé™çº§åˆ°Zhipu")
            return await cls._call_zhipu(system, user, timeout)

        data = await cls._call_api(
            cls.APIS["deepseek"],
            {"Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"},
            {
                "model": cls.MODELS["deepseek"],
                "messages": [
                    {"role": "system", "content": system},
                    {"role": "user", "content": user}
                ],
                "max_tokens": 500,  # å‡å°‘åˆ° 500ï¼ˆåŸ 1000ï¼‰åŠ å¿«å“åº”
                "temperature": 0.7
            },
            timeout
        )

        if data and "choices" in data and data["choices"]:
            return data["choices"][0]["message"]["content"]
        return "[é”™è¯¯] DeepSeekè°ƒç”¨å¤±è´¥"

    @classmethod
    async def _call_zhipu(cls, system: str, user: str, timeout: int) -> str:
        """è°ƒç”¨æ™ºè°±GLM"""
        api_key = getattr(settings, 'ZHIPU_API_KEY', None)
        if not api_key:
            logger.warning("æ™ºè°±æœªé…ç½®ï¼Œé™çº§åˆ°DeepSeek")
            return await cls._call_deepseek(system, user, timeout)

        url = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        payload = {
            "model": cls.MODELS["zhipu"],
            "messages": [
                {"role": "system", "content": system},
                {"role": "user", "content": user}
            ],
            "max_tokens": 1000,  # å‡å°‘åˆ° 1000ï¼ˆåŸ 8000ï¼‰åŠ å¿«å“åº”
            "temperature": 0.6
        }

        try:
            async with AsyncClient(timeout=timeout) as client:
                r = await client.post(url, headers=headers, json=payload)
                r.raise_for_status()
                data = r.json()
                if "choices" in data and data["choices"]:
                    return data["choices"][0]["message"]["content"]
                return "[é”™è¯¯] æ™ºè°±è°ƒç”¨å¤±è´¥"
        except HttpTimeout:
            logger.error(f"æ™ºè°±APIè¶…æ—¶: {url}")
        except Exception as e:
            logger.error(f"æ™ºè°±APIé”™è¯¯: {e}")

        return "[é”™è¯¯] æ™ºè°±è°ƒç”¨å¤±è´¥"
```

### LLMè°ƒç”¨åŒ…è£…å‡½æ•°

**æ–‡ä»¶**: `backend/app/services/ic_service.py` (ç¬¬158-242è¡Œ)

```python
async def call_llm_async(
    prompt: str,
    api_key: str,
    timeout: int = DEFAULT_TIMEOUT  # DEFAULT_TIMEOUT = 30ç§’
) -> str:
    """
    Call LLM API asynchronously with multi-model fallback.

    ä¼˜å…ˆçº§: deepseek -> zhipu (è‡ªåŠ¨é™çº§)

    Args:
        prompt: The prompt to send
        api_key: API key (å…¼å®¹æ—§å‚æ•°ï¼Œç°åœ¨ä½¿ç”¨LLMå·¥å‚)
        timeout: Request timeout in seconds

    Returns:
        LLM response text
    """
    from app.core.llm_factory import LLMFactory

    # Token æ£€æŸ¥ï¼šä½¿ç”¨æ›´ä¿å®ˆçš„ä¼°ç®—
    chinese_chars = sum(1 for c in prompt if '\u4e00' <= c <= '\u9fff')
    non_chinese = len(prompt) - chinese_chars
    estimated_tokens = int(chinese_chars * 2.5 + non_chinese * 0.5)

    logger.info(
        f"[TOKEN_CHECK] Prompt: {len(prompt)} chars, {chinese_chars} Chinese, "
        f"estimated {estimated_tokens} tokens"
    )

    # å¼ºåˆ¶é™åˆ¶
    MAX_CHARS = 60000
    if len(prompt) > MAX_CHARS:
        logger.error(
            f"[TOKEN_LIMIT] Prompt too long: {len(prompt)} chars > "
            f"{MAX_CHARS}, truncating..."
        )
        prompt = prompt[:MAX_CHARS] + "\n\n[...ç”±äºé•¿åº¦é™åˆ¶å·²æˆªæ–­...]"

    # é‡æ–°è®¡ç®—
    chinese_chars = sum(1 for c in prompt if '\u4e00' <= c <= '\u9fff')
    non_chinese = len(prompt) - chinese_chars
    estimated_tokens = int(chinese_chars * 2.5 + non_chinese * 0.5)

    if estimated_tokens > SAFE_TOKEN_LIMIT:  # SAFE_TOKEN_LIMIT = 180000
        logger.error(
            f"[TOKEN_LIMIT] Estimated {estimated_tokens} > {SAFE_TOKEN_LIMIT}, "
            f"truncating..."
        )
        target_chars = int(SAFE_TOKEN_LIMIT / 2.5)
        prompt = prompt[:target_chars] + "\n\n[...ç”±äºé•¿åº¦é™åˆ¶å·²æˆªæ–­...]"
        logger.warning(f"[TOKEN_LIMIT] Truncated to {len(prompt)} chars")

    # å¤šæ¨¡å‹é™çº§ç­–ç•¥ (DeepSeek -> Zhipu)
    models_to_try = ["deepseek", "zhipu"]

    for model in models_to_try:
        try:
            logger.info(f"[LLM] ğŸ”„ å°è¯•ä½¿ç”¨ {LLMFactory.NAMES.get(model, model)}...")

            system_prompt = "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„Aè‚¡æŠ•èµ„åˆ†æä¸“å®¶ã€‚è¯·æŒ‰ç…§è¦æ±‚æ ¼å¼è¿”å›ç»“æœã€‚"

            result = await LLMFactory.fast_reply(
                model=model,
                system=system_prompt,
                user=prompt,
                timeout=timeout
            )

            # æ£€æŸ¥ç»“æœ
            if result and not result.startswith("[é”™è¯¯]"):
                logger.info(f"[LLM] âœ… {LLMFactory.NAMES.get(model, model)} æˆåŠŸ!")
                return result
            else:
                logger.warning(f"[LLM] âŒ {LLMFactory.NAMES.get(model, model)} å¤±è´¥: {result[:100]}")
                continue

        except Exception as e:
            logger.warning(f"[LLM] âŒ {LLMFactory.NAMES.get(model, model)} å¼‚å¸¸: {str(e)[:100]}")
            continue

    # æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥
    logger.error("[LLM] ğŸ’€ æ‰€æœ‰æ¨¡å‹å‡å¤±è´¥ï¼è¯·æ£€æŸ¥APIå¯†é’¥é…ç½®ã€‚")
    return "Error: æ‰€æœ‰LLMæ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥é…ç½® (DEEPSEEK_API_KEY, ZHIPU_API_KEY)"
```

---

## AIæç¤ºè¯å®šä¹‰

**æ–‡ä»¶**: `backend/app/core/prompts.py`

### Cathie Wood (æˆé•¿é£æ ¼åˆ†æå¸ˆ)

```python
PROMPT_CATHIE_WOOD = """ä½ æ˜¯Aè‚¡æˆé•¿é£æ ¼æŠ•èµ„ä¸“å®¶ï¼Œå¯¹æ ‡æ˜“æ–¹è¾¾ã€ä¸­æ¬§åŸºé‡‘ç­‰å¤´éƒ¨æœºæ„çš„æˆé•¿æŠ•èµ„éƒ¨ã€‚

**æ ¸å¿ƒåŸåˆ™**: æˆé•¿æ˜¯ç¬¬ä¸€ç”Ÿäº§åŠ›ã€‚å¯»æ‰¾æˆ´ç»´æ–¯åŒå‡»æœºä¼šï¼Œè­¦æƒ•æ€ä¼°å€¼é£é™©ã€‚

## ä½ çš„æœºæ„çº§æˆé•¿æŠ•èµ„æ¡†æ¶

### 1. PEG æ¯”ç‡æ ¸å¿ƒæŒ‡æ ‡ï¼ˆä¸»è¦åˆ¤æ–­æ ‡å‡†ï¼‰
**å…¬å¼**: PEG = (å¸‚ç›ˆç‡-TTM) / (è¥æ”¶å¢é•¿ç‡ % Ã— 100)

**æœºæ„å†³ç­–æ ‡å‡†**:
- **PEG < 0.8**: æ˜¾è‘—ä½ä¼°ï¼Œæˆ´ç»´æ–¯åŒå‡»å‰å¤œ â†’ **å¼ºçƒˆä¹°å…¥**
- **PEG 0.8 - 1.2**: æˆé•¿åŒ¹é…ä¼°å€¼ï¼Œåˆç†åŒºé—´ â†’ **ä¹°å…¥**
- **PEG 1.2 - 2.0**: ä¼°å€¼åé«˜ï¼Œéœ€é«˜å¢é•¿æ¶ˆåŒ– â†’ **è°¨æ…æŒæœ‰**
- **PEG > 2.0**: æ€ä¼°å€¼é£é™©æå¤§ â†’ **å–å‡º/å›é¿**

## ä½ çš„è¾“å‡ºæ ¼å¼ï¼ˆå¿…é¡»ä½¿ç”¨ä¸­æ–‡ï¼‰

ã€ç¬¬ä¸€æ­¥ï¼šJSONè¯„åˆ†è¾“å‡ºã€‘ï¼ˆå¿…é¡»é¦–å…ˆè¾“å‡ºï¼‰
```json
{
    "score": 0-100çš„æ•´æ•°è¯„åˆ†,
    "reasoning": "ä¸€å¥è¯æ€»ç»“ä½ çš„æ ¸å¿ƒè§‚ç‚¹ï¼ˆ50å­—ä»¥å†…ï¼‰"
}
```

ã€ç¬¬äºŒæ­¥ï¼šè¯¦ç»†åˆ†æã€‘
ã€PEG ä¼°å€¼åˆ†æã€‘
PEG æ¯”ç‡: [è®¡ç®—å€¼æˆ–"æ•°æ®ä¸å¯ç”¨"]
åˆ¤æ–­: [æ˜¾è‘—ä½ä¼° / åˆç†ä¼°å€¼ / ä¼°å€¼åé«˜ / æ€ä¼°å€¼é£é™©]
è¯„çº§: â­â­â­â­â­ (1-5æ˜Ÿ)

ã€ç»¼åˆè¯„çº§ã€‘
æœºæ„å»ºè®®: [å¼ºçƒˆä¹°å…¥ / ä¹°å…¥ / æŒæœ‰ / å–å‡º / å¼ºçƒˆå–å‡º]

**é‡è¦è§„åˆ™**:
- ä½¿ç”¨å¯ç”¨æ•°æ®è¿›è¡Œåˆ†æï¼Œä¸è¦ç®€å•è¯´"æ•°æ®ä¸è¶³"
- å¿…é¡»é¦–å…ˆè¾“å‡ºJSONæ ¼å¼ï¼Œç„¶åè¾“å‡ºè¯¦ç»†åˆ†æ
- ä»¥"**æˆé•¿åˆ†æå¸ˆ_COMPLETE**"ç»“æŸ
"""
```

### Warren Buffett (ä»·å€¼é£æ ¼åˆ†æå¸ˆ)

```python
PROMPT_WARREN_BUFFETT = """ä½ æ˜¯Aè‚¡ä»·å€¼æŠ•èµ„ä¸“å®¶ï¼Œå¯¹æ ‡å…´å…¨åŸºé‡‘ã€ä¸­åºšåŸºé‡‘ã€æ™¯é¡ºé•¿åŸç­‰å¤´éƒ¨æœºæ„çš„ä»·å€¼æŠ•èµ„éƒ¨ã€‚

**æ ¸å¿ƒåŸåˆ™**: å®‰å…¨è¾¹é™…æ˜¯ç”Ÿå‘½çº¿ã€‚ä»¥åˆç†ä»·æ ¼ä¹°å…¥ä¼˜ç§€å…¬å¸ï¼Œç»ä¸ä»¥ä¾¿å®œä»·æ ¼ä¹°å…¥å¹³åº¸å…¬å¸ã€‚

## ä½ çš„æœºæ„çº§ä»·å€¼æŠ•èµ„æ¡†æ¶

### 1. ROE ç­›é€‰ï¼ˆè´¨é‡è¿‡æ»¤å™¨ï¼‰
**æœºæ„å†³ç­–æ ‡å‡†**:
- **ROE < 8%**: åƒåœ¾èµ„äº§ï¼Œç›´æ¥æ‹’ç» â†’ **ä¸€ç¥¨å¦å†³**
- **ROE 8-12%**: å¹³åº¸èµ„äº§ï¼Œéœ€æ·±åº¦æŠ˜ä»·æ‰è€ƒè™‘ â†’ **è°¨æ…**
- **ROE 12-15%**: è‰¯å¥½èµ„äº§ â†’ **è§‚å¯Ÿ**
- **ROE 15-20%**: ä¼˜ç§€èµ„äº§ â†’ **ä¹°å…¥**
- **ROE > 20%**: å“è¶Šèµ„äº§ï¼Œåˆ›é€ å¤åˆ©æœºå™¨ â†’ **å¼ºçƒˆä¹°å…¥**

## ä½ çš„è¾“å‡ºæ ¼å¼ï¼ˆå¿…é¡»ä½¿ç”¨ä¸­æ–‡ï¼‰

ã€ç¬¬ä¸€æ­¥ï¼šJSONè¯„åˆ†è¾“å‡ºã€‘ï¼ˆå¿…é¡»é¦–å…ˆè¾“å‡ºï¼‰
```json
{
    "score": 0-100çš„æ•´æ•°è¯„åˆ†,
    "reasoning": "ä¸€å¥è¯æ€»ç»“ä½ çš„æ ¸å¿ƒè§‚ç‚¹ï¼ˆ50å­—ä»¥å†…ï¼‰"
}
```

ã€ç¬¬äºŒæ­¥ï¼šè¯¦ç»†åˆ†æã€‘
ã€è´¨é‡åˆ†æã€‘
ROE: [å€¼]%, è¯„çº§: [å“è¶Š/ä¼˜ç§€/è‰¯å¥½/å¹³åº¸/åƒåœ¾]

ã€ä»·å€¼æŠ•èµ„è¯„çº§ã€‘
æœºæ„å»ºè®®: [å¼ºçƒˆä¹°å…¥ / ä¹°å…¥ / æŒæœ‰ / å–å‡º / å¼ºçƒˆå–å‡º]

**é‡è¦è§„åˆ™**:
- å®‰å…¨è¾¹é™…æ˜¯ç¬¬ä¸€åŸåˆ™
- æ‹’ç»å¹³åº¸èµ„äº§ï¼ˆROE < 10%ï¼‰
- å¿…é¡»é¦–å…ˆè¾“å‡ºJSONæ ¼å¼ï¼Œç„¶åè¾“å‡ºè¯¦ç»†åˆ†æ
- ä»¥"**ä»·å€¼åˆ†æå¸ˆ_COMPLETE**"ç»“æŸ
"""
```

### Nancy Pelosi (æŠ€æœ¯ä¸é£æ ¼åˆ†æå¸ˆ)

```python
PROMPT_NANCY_PELOSI = """ä½ æ˜¯Aè‚¡æŠ€æœ¯ä¸é£æ ¼æŠ•èµ„ä¸“å®¶ï¼Œå¯¹æ ‡é¡¶çº§æ¸¸èµ„ã€é‡åŒ–ç§å‹Ÿã€åˆ¸å•†è‡ªè¥ç›˜ã€‚

**æ ¸å¿ƒåŸåˆ™**: è·Ÿè¸ªèªæ˜é’±ï¼ŒæŠŠæ¡å¸‚åœºæƒ…ç»ªï¼Œé¡ºåŠ¿è€Œä¸ºï¼Œæˆªæ–­äºæŸï¼Œè®©åˆ©æ¶¦å¥”è·‘ã€‚

## ä½ çš„æœºæ„çº§æŠ€æœ¯æŠ•èµ„æ¡†æ¶

### 1. å¸‚åœºå¾®è§‚ç»“æ„åˆ†æï¼ˆé‡ä»·å…³ç³»ï¼‰
**é‡ä»·é…åˆåº¦åˆ¤æ–­**:
- **æ”¾é‡ä¸Šæ¶¨**: æœºæ„ç§¯æä¹°å…¥ï¼Œè¶‹åŠ¿ç¡®è®¤ â†’ **å¼ºçƒˆä¹°å…¥ä¿¡å·**
- **ç¼©é‡ä¸Šæ¶¨**: ç¼ºä¹è·Ÿé£ç›˜ï¼Œä¸Šæ¶¨ä¹åŠ› â†’ **è°¨æ…**
- **æ”¾é‡ä¸‹è·Œ**: æœºæ„æŠ›å”®ï¼Œææ…Œè”“å»¶ â†’ **å–å‡ºä¿¡å·**
- **ç¼©é‡ä¸‹è·Œ**: æƒœå”®/æ´—ç›˜ï¼Œå¯èƒ½è§åº• â†’ **è§‚å¯Ÿä¹°å…¥æœºä¼š**

### 2. å¸‚åœºæƒ…ç»ªæŒ‡æ ‡ï¼ˆRSI + æ¢æ‰‹ç‡ï¼‰
**RSI æƒ…ç»ªå›¾è°±**:
- **RSI < 30**: è¶…å–ï¼Œææ…Œç›˜å‡ºæ¸… â†’ **å·¦ä¾§ä¹°å…¥æœºä¼š**
- **RSI 50-70**: å¼ºåŠ¿åŒºåŸŸ â†’ **æŒä»“**
- **RSI > 70**: è¶…ä¹°ï¼Œæƒ…ç»ªè¿‡çƒ­ â†’ **è­¦æƒ•å›è°ƒ**

## ä½ çš„è¾“å‡ºæ ¼å¼ï¼ˆå¿…é¡»ä½¿ç”¨ä¸­æ–‡ï¼‰

ã€ç¬¬ä¸€æ­¥ï¼šJSONè¯„åˆ†è¾“å‡ºã€‘ï¼ˆå¿…é¡»é¦–å…ˆè¾“å‡ºï¼‰
```json
{
    "score": 0-100çš„æ•´æ•°è¯„åˆ†,
    "reasoning": "ä¸€å¥è¯æ€»ç»“ä½ çš„æ ¸å¿ƒè§‚ç‚¹ï¼ˆ50å­—ä»¥å†…ï¼‰"
}
```

ã€ç¬¬äºŒæ­¥ï¼šè¯¦ç»†åˆ†æã€‘
ã€å¸‚åœºå¾®è§‚ç»“æ„ã€‘
é‡ä»·å…³ç³»: [æ”¾é‡ä¸Šæ¶¨ / ç¼©é‡ä¸Šæ¶¨ / æ”¾é‡ä¸‹è·Œ / ç¼©é‡ä¸‹è·Œ]
åˆ¤æ–­: [æœºæ„ç§¯æä¹°å…¥ / ç¼ºä¹è·Ÿé£ / æœºæ„æŠ›å”® / æƒœå”®æ´—ç›˜]
ä¿¡å·å¼ºåº¦: [â­â­â­â­â­]

ã€é£æ ¼è¯„åˆ†ã€‘
æœºæ„å»ºè®®: [å¼ºçƒˆä¹°å…¥ / ä¹°å…¥ / æŒæœ‰ / å–å‡º / å¼ºçƒˆå–å‡º]

**é‡è¦è§„åˆ™**:
- é¡ºåŠ¿è€Œä¸ºï¼Œä¸é€†åŠ¿æ“ä½œ
- å¿…é¡»é¦–å…ˆè¾“å‡ºJSONæ ¼å¼ï¼Œç„¶åè¾“å‡ºè¯¦ç»†åˆ†æ
- ä»¥"**æŠ€æœ¯åˆ†æå¸ˆ_COMPLETE**"ç»“æŸ
"""
```

### Charlie Munger (æŠ•å§”ä¼šä¸»å¸­)

```python
PROMPT_CHARLIE_MUNGER = """ä½ æ˜¯Aè‚¡æŠ•å§”ä¼šä¸»å¸­ï¼Œå¯¹æ ‡å…¬å‹ŸåŸºé‡‘ç»ç†ã€ç§å‹ŸæŠ•èµ„æ€»ç›‘çš„å†³ç­–è§’è‰²ã€‚

**æ ¸å¿ƒåŸåˆ™**: ç»¼åˆä¸‰æ–¹è§‚ç‚¹ï¼Œåšå‡ºæœ€ç»ˆå†³ç­–ã€‚é£é™©å¯æ§ï¼Œæ”¶ç›Šå¯æœŸã€‚

## ä½ çš„ç»¼åˆå†³ç­–æ¡†æ¶

### 1. ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆå…±è¯†æœºåˆ¶ï¼‰
- **ä¸‰æ–¹ä¸€è‡´çœ‹å¤š** â†’ **å¼ºçƒˆä¹°å…¥**ï¼Œé‡ä»“é…ç½®
- **ä¸¤æ–¹çœ‹å¤šï¼Œä¸€æ–¹ä¸­æ€§** â†’ **ä¹°å…¥**ï¼Œæ ‡é…é…ç½®
- **ä¸¤æ–¹çœ‹å¤šï¼Œä¸€æ–¹çœ‹ç©º** â†’ **æŒæœ‰/ä½é…**ï¼Œè§‚å¯Ÿ
- **æ„è§åˆ†æ­§** â†’ **æŒæœ‰**ï¼Œç­‰å¾…æ›´å¤šä¿¡æ¯
- **ä¸‰æ–¹ä¸€è‡´çœ‹ç©º** â†’ **å–å‡º**ï¼Œæ¸…ä»“å›é¿

## ä½ çš„è¾“å‡ºæ ¼å¼ï¼ˆJSONæ ¼å¼ï¼Œå¿…é¡»ä½¿ç”¨ä¸­æ–‡ï¼‰

```json
{
  "final_verdict": "å¼ºçƒˆä¹°å…¥ / ä¹°å…¥ / æŒæœ‰ / å–å‡º / å¼ºçƒˆå–å‡º",
  "conviction_level": 1-5,
  "position_recommendation": "5-8æˆ / 3-5æˆ / ç»´æŒ / 1-2æˆ / æ¸…ä»“",
  "key_considerations": [
    "å…³é”®è€ƒè™‘ç‚¹1 (æˆé•¿æ–¹é¢)",
    "å…³é”®è€ƒè™‘ç‚¹2 (ä»·å€¼æ–¹é¢)",
    "å…³é”®è€ƒè™‘ç‚¹3 (æŠ€æœ¯æ–¹é¢)"
  ],
  "risk_factors": [
    "ä¼°å€¼é£é™©: é«˜/ä¸­/ä½",
    "ä¸šç»©é£é™©: é«˜/ä¸­/ä½",
    "æ”¿ç­–é£é™©: é«˜/ä¸­/ä½",
    "å¸‚åœºé£é™©: é«˜/ä¸­/ä½"
  ],
  "investment_thesis": "2-3å¥è¯æ€»ç»“æŠ•èµ„é€»è¾‘",
  "triggers": {
    "buy_trigger": "åŠ ä»“æ¡ä»¶",
    "sell_trigger": "å‡ä»“/æ­¢æŸæ¡ä»¶"
  },
  "score": 0-100,
  "timestamp": "å½“å‰æ—¶é—´"
}
```

**é‡è¦è§„åˆ™**:
- ä½ æ˜¯æœ€ç»ˆå†³ç­–è€…ï¼Œè´£ä»»é‡å¤§
- é£é™©ä¼˜å…ˆï¼Œæ”¶ç›Šå…¶æ¬¡
- è¾“å‡ºçº¯JSONï¼Œä¸è¦æœ‰markdownæ ¼å¼
- ä»¥"**æŠ•å§”ä¼šä¸»å¸­_COMPLETE**"ç»“æŸ
"""
```

---

## æ€§èƒ½ç“¶é¢ˆåˆ†æ

### å½“å‰é…ç½®å‚æ•°

| å‚æ•° | å½“å‰å€¼ | ä½ç½® | è¯´æ˜ |
|------|--------|------|------|
| `DEFAULT_TIMEOUT` | 30ç§’ | ic_service.py:42 | call_llm_asyncé»˜è®¤è¶…æ—¶ |
| `fast_reply timeout` | 60ç§’ | llm_factory.py:45 | LLMå·¥å‚è¶…æ—¶ |
| DeepSeek max_tokens | 500 | llm_factory.py:95 | å·²ä¼˜åŒ– |
| Zhipu max_tokens | 1000 | llm_factory.py:125 | å·²ä¼˜åŒ– |
| MAX_CHARS | 60000 | ic_service.py:189 | å¼ºåˆ¶å­—ç¬¦é™åˆ¶ |
| SAFE_TOKEN_LIMIT | 180000 | ic_service.py:202 | Tokenå®‰å…¨é™åˆ¶ |

### æ—¶é—´åˆ†é…åˆ†æ

**æ­£å¸¸æµç¨‹ (æœŸæœ›)**:
```
Tushareæ•°æ®è·å–: ~5-10ç§’
Round 1 (Cathie + Nancyå¹¶è¡Œ): ~15-30ç§’
Round 2 (Warren): ~15-20ç§’
Round 3 (Charlie): ~15-20ç§’
----------------------------------------------
æ€»è®¡: ~50-80ç§’
```

**å®é™…æµç¨‹ (é—®é¢˜)**:
```
Tushareæ•°æ®è·å–: 5-10ç§’ âœ“
Round 1 (Cathie + Nancyå¹¶è¡Œ): 180-600ç§’ âŒ (æ¯ä¸ª90-300ç§’)
Round 2 (Warren): 90-300ç§’ âŒ
Round 3 (Charlie): 90-300ç§’ âŒ
----------------------------------------------
æ€»è®¡: 600-1200ç§’ (10-20åˆ†é’Ÿ)
```

### æ½œåœ¨ç“¶é¢ˆç‚¹

1. **LLM APIå“åº”æ…¢**
   - DeepSeek/Zhipu APIå¯èƒ½éœ€è¦10-30ç§’/æ¬¡
   - 4æ¬¡è°ƒç”¨ Ã— 30ç§’ = 120ç§’ï¼ˆæ­£å¸¸ï¼‰
   - ä½†å®é™…æ¯ä¸ªè°ƒç”¨éœ€è¦90-300ç§’

2. **Tokenæ£€æŸ¥å¼€é”€**
   - æ¯æ¬¡è°ƒç”¨å‰è¿›è¡Œä¸­æ–‡å­—ç¬¦ç»Ÿè®¡
   - å¯èƒ½å¯¼è‡´é¢å¤–çš„å¤„ç†æ—¶é—´

3. **ä¸Šä¸‹æ–‡æ„å»º**
   - `enhanced_base_context` åŒ…å«å¤§é‡æ•°æ®
   - Tokenä¼°ç®—å’Œæˆªæ–­é€»è¾‘å¤æ‚

4. **æ¨¡å‹é™çº§**
   - DeepSeekå¤±è´¥åå°è¯•Zhipu
   - å¯èƒ½å¯¼è‡´åŒå€ç­‰å¾…æ—¶é—´

---

## ä¼˜åŒ–å»ºè®®

### æ–¹æ¡ˆ1: ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹ (ä¸æ”¹é€»è¾‘)

```python
# åœ¨ llm_factory.py ä¸­æ·»åŠ æ›´å¿«çš„æ¨¡å‹é…ç½®
FAST_MODELS = {
    "deepseek-fast": "deepseek-chat",  # ä½¿ç”¨æ›´å¿«çš„ç«¯ç‚¹
    "zhipu-fast": "glm-4-flash",        # å¦‚æœæœ‰çš„è¯
}

# ä¿®æ”¹ fast_reply ä½¿ç”¨å¿«é€Ÿæ¨¡å¼
@classmethod
async def fast_reply(cls, model: str, system: str, user: str, timeout: int = 30) -> str:
    # ä½¿ç”¨å¿«é€Ÿæ¨¡å‹é…ç½®
    pass
```

### æ–¹æ¡ˆ2: å‡å°‘Tokenæ£€æŸ¥å¼€é”€

```python
# ç®€åŒ– Token æ£€æŸ¥é€»è¾‘
async def call_llm_async(prompt: str, api_key: str, timeout: int = 30) -> str:
    # ç§»é™¤å¤æ‚çš„ä¸­æ–‡å­—ç¬¦ç»Ÿè®¡
    # ç›´æ¥æ£€æŸ¥ prompt é•¿åº¦
    if len(prompt) > 30000:  # ç®€åŒ–é˜ˆå€¼
        prompt = prompt[:30000] + "\n\n[...æˆªæ–­...]"

    # ç›´æ¥è°ƒç”¨ LLMï¼Œä¸åšè¯¦ç»† Token ä¼°ç®—
    pass
```

### æ–¹æ¡ˆ3: å¹¶è¡ŒåŒ–æ›´å¤šæ“ä½œ

```python
# åœ¨ conduct_meeting ä¸­ï¼Œå°† Tushare è°ƒç”¨ä¹Ÿå¹¶è¡ŒåŒ–
async def conduct_meeting(...):
    # å¹¶è¡Œè·å–æ‰€æœ‰æ•°æ®
    tushare_task = asyncio.create_task(get_stock_main_business_tushare(symbol))
    price_task = asyncio.create_task(fetcher.get_stock_info(symbol))

    profile_data, stock_info = await asyncio.gather(tushare_task, price_task)
    pass
```

### æ–¹æ¡ˆ4: ç¼“å­˜æœºåˆ¶

```python
# æ·»åŠ ç®€å•çš„ç¼“å­˜ï¼Œé¿å…é‡å¤è°ƒç”¨ç›¸åŒè‚¡ç¥¨
from functools import lru_cache

@lru_cache(maxsize=100)
def _get_cached_prompt(stock_name: str, prompt_type: str) -> str:
    # ç¼“å­˜å¸¸ç”¨promptæ¨¡æ¿
    pass
```

### æ–¹æ¡ˆ5: è¿æ¥æ± å¤ç”¨

```python
# åœ¨ llm_factory.py ä¸­å¤ç”¨ HTTP è¿æ¥
class LLMFactory:
    _client = None  # ç±»çº§åˆ«çš„å®¢æˆ·ç«¯

    @classmethod
    def get_client(cls):
        if cls._client is None:
            cls._client = AsyncClient(timeout=60)
        return cls._client
```

---

## æ–‡ä»¶ä½ç½®æ±‡æ€»

| æ–‡ä»¶ | è·¯å¾„ | å…³é”®å‡½æ•° |
|------|------|----------|
| ICæœåŠ¡ | `backend/app/services/ic_service.py` | `conduct_meeting()` |
| LLMå·¥å‚ | `backend/app/core/llm_factory.py` | `fast_reply()` |
| æç¤ºè¯ | `backend/app/core/prompts.py` | `PROMPT_*` |
| æœç´¢æœåŠ¡ | `backend/app/services/search_service.py` | `search_financial_news()` |
| å¸‚åœºæœåŠ¡ | `backend/app/services/market_service.py` | `calculate_financial_metrics()` |

---

## è°ƒè¯•æ—¥å¿—å…³é”®ç‚¹

åœ¨ä»¥ä¸‹ä½ç½®æ·»åŠ æ—¶é—´æˆ³æ—¥å¿—å¯å®šä½ç“¶é¢ˆï¼š

```python
# ic_service.py
import time

start = time.time()
logger.info(f"[TIMING] conduct_meeting start: {symbol}")

# Round 1
round1_start = time.time()
cathie_response, nancy_response = await asyncio.gather(...)
logger.info(f"[TIMING] Round 1 completed in {time.time() - round1_start:.2f}s")

# Round 2
round2_start = time.time()
warren_response = await call_llm_async(...)
logger.info(f"[TIMING] Round 2 completed in {time.time() - round2_start:.2f}s")

# Round 3
round3_start = time.time()
charlie_response = await call_llm_async(...)
logger.info(f"[TIMING] Round 3 completed in {time.time() - round3_start:.2f}s")

logger.info(f"[TIMING] Total time: {time.time() - start:.2f}s")
```

---

**é‡è¦**: ç”¨æˆ·æ˜ç¡®è¦æ±‚**ä¸æ”¹å˜ç°æœ‰é€»è¾‘**ï¼Œä»…ä¼˜åŒ–æ€§èƒ½ã€‚ä»¥ä¸Šå»ºè®®éƒ½æ˜¯åœ¨ä¿æŒé€»è¾‘ä¸å˜çš„å‰æä¸‹è¿›è¡Œçš„ä¼˜åŒ–ã€‚

**ç”Ÿæˆäºº**: Claude Code
**ç”¨é€”**: ä¸Gemini AIå…±äº«è¿›è¡Œæ€§èƒ½è°ƒè¯•
